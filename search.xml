<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度学习的激活函数]]></title>
    <url>%2F2019%2F07%2F09%2Factivation_functions_in_deeplearning%2F</url>
    <content type="text"><![CDATA[说明 本文对深度学习中的重要组件——激活函数做系统性汇总。 了解激活函数 什么是激活函数 在神经网络中，一个节点的激活函数(Activation Function)定义了该节点在给定的输入变量或输入集合下的输出。wiki中以计算机芯片电路为例，标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。激活函数主要用于提升神经网络解决非线性问题的能力。激活函数各式各样，各有优缺点，目前常用的有 ReLU、sigmoid、tanh等。各个激活函数的细节详见下文。 为什么需要激活函数 当不用激活函数时，神经网络的权重和偏差只会进行线性变换。线性方程很简单，但是解决复杂问题的能力有限。没有激活函数的神经网络实质上就是一个线性回归模型。为了方便理解，以一个简单的例子来说明。考虑如下网络 在不用激活函数的情况下，该图可用如下公式表示 \[output = w7(input1*w1 +input2*w2)+w8(input1*w3+input2*w4)+w9(input1*w5+input2*w6)\] 实质是\(output=\begin{bmatrix} w1*w7+w3*w8+w5*w9 \\ w2*w7+w4*w8+w6*w9 \end{bmatrix} * \begin{bmatrix}input1 \\ input2 \end{bmatrix} \implies Y=WX\)的线性方程。 若在隐藏层引入激活函数\(h(y)\)，例如令\(h(y) = max(y,0)\)，那么原始式子就无法用简单线性方程表示了。 \[output=w7*max(input1*w1 +input2*w2,0)+w8*max(input1*w3+input2*w4,0)+w9*max(input1*w5+input2*w6,0)\] 激活函数类型 wiki中给出了激活函数的类型，这里略做归纳 * Identity function (恒等函数) * Step function (阶跃函数) * Sigmoidal function (S形函数) * Ramp function (斜坡函数) Binary 和 Bipolar 的区别应该是 Binary（单位）对应值为 0 或 1， Bipolar（两极）对应值为 -1 或 1 激活函数的一些特性 非线性(Nonlinear) 当激活函数是非线性的，那么一个两层神经网络也证明是一个通用近似函数通用近似理论。而恒等激活函数则无法满足这一特性，当多层网络的每一层都是恒等激活函数时，该网络实质等同于一个单层网络。 输出值域(Range) 若激活函数的值域是有限的，因为对权重的更新影响有限，所以基于梯度的训练方法更稳定。若值域是无限的，因为大多数情况下权重更新更明显，所以训练通常更有效，通常需要设定较小的学习率。 连续可微(Continuously differentiable) 通常情况下，当激活函数连续可微，则可以用基于梯度的优化方法。（也有例外，如ReLU函数虽不是连续可微，使用梯度优化也存在一些问题，如ReLU存在由于梯度过大或学习率过大而导致某个神经元输出小于0，从而使得该神经元输出始终是0，并且无法对与其相连的神经元参数进行更新，相当于该神经元进入了“休眠”状态，参考深度学习中，使用relu存在梯度过大导致神经元“死亡”，怎么理解？ - 知乎，但ReLU还是可以使用梯度优化的。）二值阶跃函数在0处不可微，并且在其他地方的导数是零，所以梯度优化方法不适用于该激活函数。 单调(Monotonic) 当激活函数为单调函数时，单层模型的误差曲面一定是凸面。即对应的误差函数是凸函数，求得的最小值一定是全局最小值。 一阶导单调(Smooth functions with a monotonic derivative) 通常情况下，这些函数表现更好。 原点近似恒等函数(Approximates identity near the origin) 若激活函数有这一特性，神经网络在随机初始化较小的权重时学习更高效。若激活函数不具备这一特性，初始化权重时必须特别小心。参考machine learning - Why activation functions that approximate the identity near origin are preferable? - Cross Validated 激活函数速查表 以下是输入为一个变量的激活函数列表，主要参考自 wiki 函数 图形 值域 连续性 单调 一阶导单调 原点近似恒等 Identity(恒等函数) \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{\infty }}\) 是 是 是 Binary step(单位阶跃函数) \({\displaystyle \{0,1\}}\) \({\displaystyle C^{-1}}\) 是 否 否 Sigmoid(S函数又称Logistic逻辑函数) \({\displaystyle (0,1)}\) \({\displaystyle C^{\infty }}\) 是 否 否 TanH(双曲正切函数) \({\displaystyle (-1,1)}\) \({\displaystyle C^{\infty }}\) 是 否 是 ArcTan(反正切函数) \({\displaystyle \left(-{\frac {\pi }{2}},{\frac {\pi }{2}}\right)}\) \({\displaystyle C^{\infty }}\) 是 否 是 Softsign函数 \({\displaystyle (-1,1)}\) \({\displaystyle C^{1}}\) 是 否 是 Inverse square root unit(反平方根函数,ISRU) \({\displaystyle \left(-{\frac {1}{\sqrt {\alpha }}},{\frac {1}{\sqrt {\alpha }}}\right)}\) \({\displaystyle C^{\infty }}\) 是 否 是 Inverse square root linear unit(反平方根线性函数,ISRLU) \({\displaystyle \left(-{\frac {1}{\sqrt {\alpha }}},\infty \right)}\) \({\displaystyle C^{2}}\) 是 是 是 Square Nonlinearity(平方非线性函数,SQNL) \({\displaystyle (-1,1)}\) \({\displaystyle C^{2}}\) 是 否 是 Rectified linear unit(线性整流函数,ReLU) \({\displaystyle [0,\infty )}\) \({\displaystyle C^{0}}\) 是 是 否 Bipolar rectified linear unit(二级线性整流函数,BReLU) \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{0}}\) 是 是 否 Leaky rectified linear unit(带泄露随机线性整流函数,Leaky ReLU) \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{0}}\) 是 是 否 Parameteric rectified linear unit(参数化线性整流函数,PReLU) \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{0}}\) 是 iff \({\displaystyle \alpha \geq 0}\) 是 是 iff \({\displaystyle \alpha = 1}\) Randomized leaky rectified linear unit(带泄露随机线性整流函数,RReLU) \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{0}}\) 是 是 否 Exponential linear unit(指数线性函数,ELU) \({\displaystyle (-\alpha ,\infty )}\) \({\displaystyle {\begin{cases}C_{1}&amp;{\text{when }}\alpha =1\\C_{0}&amp;{\text{otherwise }}\end{cases}}}\) 是 iff \({\displaystyle \alpha \geq 0}\) 是 iff \({\displaystyle 0\leq \alpha \leq 1}\) 是 iff \({\displaystyle \alpha = 1}\) Scaled exponential linear unit(扩展指数线性函数,SELU) \({\displaystyle (-\lambda \alpha ,\infty )}\) \({\displaystyle C^{0}}\) 是 否 否 S-shaped rectified linear activation unit(S型线性整流激活函数,SReLU) \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{0}}\) 否 否 否 Adaptive piecewise linear(自适应分段线性函数,APL) \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{0}}\) 否 否 否 SoftPlus函数 \({\displaystyle (0,\infty )}\) \({\displaystyle C^{\infty }}\) 是 是 否 Bent identity(弯曲恒等函数) \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{\infty }}\) 是 是 是 Sigmoid-weighted linear unit (SiLU)[11] (也被称为Swish[12]) \({\displaystyle [\approx -0.28,\infty )}\) \({\displaystyle C^{\infty }}\) 否 否 否 SoftExponential函数 \({\displaystyle (-\infty ,\infty )}\) \({\displaystyle C^{\infty }}\) 是 是 是 iff \({\displaystyle \alpha = 0}\) Soft Clipping(柔性剪峰函数) \({\displaystyle (0,1)}\) \({\displaystyle C^{\infty }}\) 是 否 否 Sinusoid(正弦函数) \({\displaystyle [-1,1]}\) \({\displaystyle C^{\infty }}\) 否 否 是 Sinc函数 \({\displaystyle [\approx -.217234,1]}\) \({\displaystyle C^{\infty }}\) 否 否 否 Gaussian(高斯函数) \({\displaystyle (0,1]}\) \({\displaystyle C^{\infty }}\) 否 否 否 Hard Sigmoid(分段近似Sigmoid函数) \([0,1]\) \(C^{0}\) 是 否 否 Hard Tanh(分段近似Tanh函数) \([-1,1]\) \(C^{0}\) 是 否 是 LeCun Tanh(也称Scaled Tanh,按比例缩放的Tanh函数) \((-1.7519 , 1.7519)\) \(C^{\infty }\) 是 否 否 Symmetrical Sigmoid(对称Sigmoid函数) \((-1, -1)\) \(C^{\infty }\) 是 否 否 Complementary Log Log函数 \((0, 1)\) \(C^{\infty }\) 是 否 否 Absolute(绝对值函数) \([0, \infty)\) \(C^{0 }\) 否 否 否 激活函数详细描述 函数图形说明 函数图形主要取自Visualising Activation Functions in Neural Networks。 翻译版本访问这里 左侧蓝线是激活函数方程式图形，黄色是激活函数一阶导图形。 右侧篮框是对激活函数方程式的各项特性描述，包括： - Range(激活函数输出值域) - Monotonic(激活函数是否单调) - Continuity(激活函数连续性类型) - Identity at Origin(激活函数在原点处是否近似恒等) - Symmetry(激活函数是否对称) 右侧黄框是对激活函数一阶导的各项特性描述，包括： - Range(一阶导输出值域) - Monotonic(一阶导是否单调) - Continuous(一阶导是否连续) - Vanishing Gradient(是否梯度消失:指随着网络向后传递，是否存在当梯度值过小时，梯度变化以指数形式衰减，直至消失，导致后面的网络节点几乎无法更新参数) - Exploding Gradient(是否梯度爆炸:指是否存在当梯度值过大，梯度变化以指数形式增加，直至爆炸) - Saturation(饱和:指激活函数值接近其边界时，如sigmoid函数值接近0或1时，函数曲线是否平缓，过于平缓可能会导致在向后传播时梯度消失) - Dead Neurons(神经元死亡:指在网络传播时，是否会出现某个神经元永远不会再更新的情况) Identity(恒等函数) 描述： 一种输入和输出相等的激活函数，比较适合线性问题，如线性回归问题。但不适用于解决非线性问题。 方程式： \({\displaystyle f(x)=x}\) 一阶导： \({\displaystyle f&#39;(x)=1}\) 图形： Binary step(单位阶跃函数) 描述： step与神经元激活的含义最贴近，指当刺激超过阈值时才会激发。但是由于该函数的梯度始终为0，不能作为深度网络的激活函数 方程式： \({\displaystyle f(x)={\begin{cases}0&amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\) 一阶导： \({\displaystyle f^{\prime}(x)={\begin{cases}0&amp;{\text{for }}x\neq 0\\?&amp;{\text{for }}x=0\end{cases}}}\) 图形： Sigmoid(S函数又称Logistic逻辑函数) 描述： 使用很广的一类激活函数，具有指数函数形状，在物理意义上最接近生物神经元。并且值域在(0,1)之间，可以作为概率表示。该函数也通常用于对输入的归一化，如Sigmoid交叉熵损失函数。Sigmoid激活函数具有梯度消失和饱和的问题，一般来说，sigmoid网络在5层之内就会产生梯度消失现象。 方程式： \({\displaystyle f(x)=\sigma (x)={\frac {1}{1+e^{-x}}}}\) 一阶导： \({\displaystyle f&#39;(x)=f(x)(1-f(x))}\) 图形： TanH(双曲正切函数) 描述： TanH与Sigmoid函数类似，在输入很大或很小时，输出几乎平滑，梯度很小，不利于权重更新，容易出现梯度消失和饱和的问题。不过TanH函数值域在(-1,1)之间，以0为中心反对称，且原点近似恒等，这些点是加分项。一般二分类问题中，隐藏层用tanh函数，输出层用sigmod函数。 方程式： \({\displaystyle f(x)=\tanh(x)={\frac {(e^{x}-e^{-x})}{(e^{x}+e^{-x})}}}\) 一阶导： \({\displaystyle f&#39;(x)=1-f(x)^{2}}\) 图形： ArcTan(反正切函数) 描述： ArcTen从图形上看类似TanH函数，只是比TanH平缓，值域更大。从一阶导看出导数趋于零的速度比较慢，因此训练比较快。 方程式： \({\displaystyle f(x)=\tan ^{-1}(x)}\) 一阶导： \({\displaystyle f&#39;(x)={\frac {1}{x^{2}+1}}}\) 图形： Softsign函数 描述： Softsign从图形上看也类似TanH函数，以0为中心反对称，训练比较快。 方程式： \({\displaystyle f(x)={\frac {x}{1+\|x\|}}}\) 一阶导： \({\displaystyle f&#39;(x)={\frac {1}{(1+\|x\|)^{2}}}}\) 图形： Inverse square root unit(反平方根函数,ISRU) 描述： 图形类似于tanH和Sigmoid函数，论文说可作为RNN层的激活函数，号称在RNN层中达到与tanh和sigmoid一样效果的情况下，计算复杂度更低。 方程式： \({\displaystyle f(x)={\frac {x}{\sqrt {1+\alpha x^{2}}}}}\) 一阶导： \({\displaystyle f&#39;(x)=\left({\frac {1}{\sqrt {1+\alpha x^{2}}}}\right)^{3}}\) 图形： Inverse square root linear unit(反平方根线性函数,ISRLU) 描述： 一个分段函数，小于0部分是ISRU，大于等于0部分是恒等函数，论文里说这函数在CNN层上相较于ReLU学习更快，更一般化。 方程式： \({\displaystyle f(x)={\begin{cases}{\frac {x}{\sqrt {1+\alpha x^{2}}}}&amp;{\text{for }}x&lt;0\\x&amp;{\text{for }}x\geq 0\end{cases}}}\) 一阶导： \({\displaystyle f&#39;(x)={\begin{cases}\left({\frac {1}{\sqrt {1+\alpha x^{2}}}}\right)^{3}&amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\) 图形： Square Nonlinearity(平方非线性函数,SQNL) 描述： 类似Softsign的函数，论文摘要中将这个与Softsign对比，说该函数在收敛速度上更快。 方程式： \({\displaystyle f(x)={\begin{cases}1&amp;:x&gt;2.0\\x-{\frac {x^{2}}{4}}&amp;:0\leq x\leq 2.0\\x+{\frac {x^{2}}{4}}&amp;:-2.0\leq x&lt;0\\-1&amp;:x&lt;-2.0\end{cases}}}\) 一阶导： \({\displaystyle f&#39;(x)=1\mp {\frac {x}{2}}}\) 图形： Rectified linear unit(线性整流函数,ReLU) 描述： 比较流行的激活函数，该函数保留了类似step那样的生物学神经元机制，即高于0才激活，不过因在0以下的导数都是0，可能会引起学习缓慢甚至神经元死亡的情况。 方程式： \({\displaystyle f(x)={\begin{cases}0&amp;{\text{for }}x\leq 0\\x&amp;{\text{for }}x&gt;0\end{cases}}}\) 一阶导： \({\displaystyle f&#39;(x)={\begin{cases}0&amp;{\text{for }}x\leq 0\\1&amp;{\text{for }}x&gt;0\end{cases}}}\) 图形： Bipolar rectified linear unit(二级线性整流函数,BReLU) 描述： relu系列的一种激活函数，形式很奇怪，采用mod 2作为条件分段，论文说在个别场景上能取得更好的效果。 方程式： \(f(x_{i})={\begin{cases}ReLU(x_{i})&amp;{\text{if }}i{\bmod {2}}=0\\-ReLU(-x_{i})&amp;{\text{if }}i{\bmod {2}}\neq 0\end{cases}}\) 一阶导： \({\displaystyle f&#39;(x_{i})={\begin{cases}ReLU&#39;(x_{i})&amp;{\text{if }}i{\bmod {2}}=0\\-ReLU&#39;(-x_{i})&amp;{\text{if }}i{\bmod {2}}\neq 0\end{cases}}}\) 图形： Leaky rectified linear unit(带泄露随机线性整流函数,Leaky ReLU) 描述： relu的一个变化，即在小于0部分不等于0，而是加一个很小的不为零的斜率，减少神经元死亡带来的影响。 方程式： \({ f(x)={\begin{cases}0.01x&amp;{\text{for }}x&lt;0\\x&amp;{\text{for }}x\geq 0\end{cases}}}\) 一阶导： \({\displaystyle f&#39;(x)={\begin{cases}0.01&amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\) 图形： Parameteric rectified linear unit(参数化线性整流函数,PReLU) 描述： 也是ReLU的一个变化，与Leaky ReLU类似，只不过PReLU将小于零部分的斜率换成了可变参数α。这种变化使值域会依据α不同而不同。 方程式： \({\displaystyle f(\alpha ,x)={\begin{cases}\alpha x&amp;{\text{for }}x&lt;0\\x&amp;{\text{for }}x\geq 0\end{cases}}}\) 一阶导： \({ f&#39;(\alpha ,x)={\begin{cases}\alpha &amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\) 图形： Randomized leaky rectified linear unit(带泄露随机线性整流函数,RReLU) 描述： 在PReLU基础上将α变成了随机数。参考论文 方程式： \({\displaystyle f(\alpha ,x)={\begin{cases}\alpha x&amp;{\text{for }}x&lt;0\\x&amp;{\text{for }}x\geq 0\end{cases}}}\) 一阶导： \({ f&#39;(\alpha ,x)={\begin{cases}\alpha &amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\) 图形： Exponential linear unit(指数线性函数,ELU) 描述： ELU小于零的部分采用了负指数形式，相较于ReLU权重可以有负值，并且在输入取较小值时具有软饱和的特性，提升了对噪声的鲁棒性。参考ELU激活函数的提出 方程式： \({\displaystyle f(\alpha ,x)={\begin{cases}\alpha (e^{x}-1)&amp;{\text{for }}x\leq 0\\x&amp;{\text{for }}x&gt;0\end{cases}}}\) 一阶导： \({ f&#39;(\alpha ,x)={\begin{cases}f(\alpha ,x)+\alpha &amp;{\text{for }}x\leq 0\\1&amp;{\text{for }}x&gt;0\end{cases}}}\) 图形： Scaled exponential linear unit(扩展指数线性函数,SELU) 描述： ELU的一种变化，引入超参λ和α，并给出了相应取值，这些取值在原论文中（Self-Normalizing Neural Networks）详细推导过程 方程式： ${ f(,x)=} $ \(with \quad{\displaystyle \lambda =1.0507} \quad and \quad {\displaystyle \alpha =1.67326}\) 一阶导： \({\displaystyle f&#39;(\alpha ,x)=\lambda {\begin{cases}\alpha (e^{x})&amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\) 图形： S-shaped rectified linear activation unit(S型线性整流激活函数,SReLU) 描述： 也是ReLU的一种变化，不同的是该函数有三个分段，四个超参数，这种设置使函数图形看起来像S型，具体参考论文。 方程式： \({\displaystyle f_{t_{l},a_{l},t_{r},a_{r}}(x)={\begin{cases}t_{l}+a_{l}(x-t_{l})&amp;{\text{for }}x\leq t_{l}\\x&amp;{\text{for }}t_{l}&lt;x&lt;t_{r}\\t_{r}+a_{r}(x-t_{r})&amp;{\text{for }}x\geq t_{r}\end{cases}}}\) 一阶导： \({\displaystyle f&#39;_{t_{l},a_{l},t_{r},a_{r}}(x)={\begin{cases}a_{l}&amp;{\text{for }}x\leq t_{l}\\1&amp;{\text{for }}t_{l}&lt;x&lt;t_{r}\\a_{r}&amp;{\text{for }}x\geq t_{r}\end{cases}}}\) 图形： Adaptive piecewise linear(自适应分段线性函数,APL) 描述： 原论文表示通过分段为神经元学习加入自适应激活功能，比ReLU在cifar-10、cifar-100上有更好的性能。 方程式： \({\displaystyle f(x)=\max(0,x)+\sum _{s=1}^{S}a_{i}^{s}\max(0,-x+b_{i}^{s})}\) 一阶导： \({\displaystyle f&#39;(x)=H(x)-\sum _{s=1}^{S}a_{i}^{s}H(-x+b_{i}^{s})}\) 图形： SoftPlus函数 描述： 是ReLU的平滑替代，函数在任何地方连续且值域非零，避免了死神经元。不过因不对称且不以零为中心，可以影响网络学习。由于导数必然小于1，所以也存在梯度消失问题。 方程式： \({\displaystyle f(x)=\ln(1+e^{x})}\) 一阶导： \({\displaystyle f&#39;(x)={\frac {1}{1+e^{-x}}}}\) 图形： Bent identity(弯曲恒等函数) 描述： 可以理解为identity和ReLU之间的一种折中，不会出现死神经元的问题，不过存在梯度消失和梯度爆炸风险。 方程式： \(\begin{align*}f(x)=\frac{\sqrt{x^2 + 1} - 1}{2} + x \end{align*}\) 一阶导： \({\displaystyle f&#39;(x)={\frac {x}{2{\sqrt {x^{2}+1}}}}+1}\) 图形： Sigmoid-weighted linear unit (SiLU) 描述： 具体参考论文 方程式： \({\displaystyle f(x)=x\cdot \sigma (x)}\) 一阶导： \({\displaystyle f&#39;(x)=f(x)+\sigma (x)(1-f(x))}\) 图形： SoftExponential函数 描述： 参考论文 方程式： \({\displaystyle f(\alpha ,x)={\begin{cases}-{\frac {\ln(1-\alpha (x+\alpha ))}{\alpha }}&amp;{\text{for }}\alpha &lt;0\\x&amp;{\text{for }}\alpha =0\\{\frac {e^{\alpha x}-1}{\alpha }}+\alpha &amp;{\text{for }}\alpha &gt;0\end{cases}}}\) 一阶导： \({\displaystyle f&#39;(\alpha ,x)={\begin{cases}{\frac {1}{1-\alpha (\alpha +x)}}&amp;{\text{for }}\alpha &lt;0\\e^{\alpha x}&amp;{\text{for }}\alpha \geq 0\end{cases}}}\) 图形： Soft Clipping(柔性剪峰函数) 描述： 参考论文 方程式： \({\displaystyle f(\alpha ,x)={\frac {1}{\alpha }}\log {\frac {1+e^{\alpha x}}{1+e^{\alpha (x-1)}}}}\) 一阶导： \({\displaystyle f&#39;(\alpha ,x)={\frac {1}{2}}\sinh \left({\frac {p}{2}}\right){\text{sech}}\left({\frac {px}{2}}\right){\text{sech}}\left({\frac {p}{2}}(1-x)\right)}\) 图形： Sinusoid(正弦函数) 描述： Sinusoid作为激活函数，为神经网络引入了周期性，且该函数处处联系，以零点对称。参考论文 方程式： \({\displaystyle f(x)=\sin(x)}\) 一阶导： \({\displaystyle f&#39;(x)=\cos(x)}\) 图形： Sinc函数 描述： Sinc函数在信号处理中尤为重要，因为它表征了矩形函数的傅立叶变换。作为激活函数，它的优势在于处处可微和对称的特性，不过容易产生梯度消失的问题。 方程式： \({\displaystyle f(x)={\begin{cases}1&amp;{\text{for }}x=0\\{\frac {\sin(x)}{x}}&amp;{\text{for }}x\neq 0\end{cases}}}\) 一阶导： \({\displaystyle f&#39;(x)={\begin{cases}0&amp;{\text{for }}x=0\\{\frac {\cos(x)}{x}}-{\frac {\sin(x)}{x^{2}}}&amp;{\text{for }}x\neq 0\end{cases}}}\) 图形： Gaussian(高斯函数) 描述： 高斯激活函数不常用。 方程式： \({\displaystyle f(x)=e^{-x^{2}}}\) 一阶导： \({\displaystyle f&#39;(x)=-2xe^{-x^{2}}}\) 图形： Hard Sigmoid(分段近似Sigmoid函数) 描述： 是Sigmoid函数的分段线性近似，更容易计算，不过存在梯度消失和神经元死亡的问题 方程式： \(f(x) =\begin{cases}0 &amp; \text{for } x&lt;-2.5\\0.2x + 0.5 &amp; \text{for } -2.5\geq x\leq 2.5 \\1 &amp; \text{for } x&gt;2.5\end{cases}\) 一阶导： \(f&#39;(x) =\begin{cases}0 &amp; \text{for } x&lt;-2.5\\0.2 &amp; \text{for } -2.5\geq x\leq 2.5 \\0 &amp; \text{for } x&gt;2.5\end{cases}\) 图形： Hard Tanh(分段近似Tanh函数) 描述： Tanh激活函数的分段线性近似。 方程式： \(f(x) =\begin{cases}-1 &amp; \text{for } x&lt;-1\\x &amp; \text{for } -1\geq x\leq 1 \\1 &amp; \text{for } x&gt;1 \end{cases}\) 一阶导： \(f&#39;(x) =\begin{cases}0 &amp; \text{for } x&lt;-1\\1 &amp; \text{for } -1\geq x\leq 1 \\0 &amp; \text{for } x&gt;1\end{cases}\) 图形： LeCun Tanh(也称Scaled Tanh,按比例缩放的Tanh函数) 描述： Tanh的缩放版本，参考论文 方程式： \(\begin{align*}f(x)&amp;=1.7519\tanh(\frac{2}{3}x)\end{align*}\) 一阶导： \(\begin{align*}f&#39;(x)&amp;=1.7519*\frac{2}{3}(1-\tanh^2(\frac{2}{3}x))\\&amp;=1.7519*\frac{2}{3}-\frac{2}{3*1.7519}f(x)^2\end{align*}\) 图形： Symmetrical Sigmoid(对称Sigmoid函数) 描述： 是Tanh的一种替代方法，比Tanh形状更扁平，导数更小，下降更缓慢。 方程式： \(\begin{align*}f(x)&amp;=\tanh(x/2)\\&amp;=\frac{1-e^{-x}}{1+e^{-x}}\end{align*}\) 一阶导： \(\begin{align*}f&#39;(x)&amp;=0.5(1-\tanh^2(x/2))\\&amp;=0.5(1-f(x)^2)\end{align*}\) 图形： Complementary Log Log函数 描述： 是Sigmoid的一种替代，相较于Sigmoid更饱和。 方程式： \(f(x)=1-e^{-e^{x}}\) 一阶导： \(\begin{align*}f&#39;(x)=e^{x}(e^{-e^{x}}) = e^{x-e^{x}}\end{align*}\) 图形： Absolute(绝对值函数) 描述： 导数只有两个值。 方程式： \(\begin{align*}f(x)=|x|\end{align*}\) 一阶导： \(f&#39;(x) =\begin{cases}-1 &amp; \text{for } x&lt;0\\1 &amp; \text{for } x&gt;0\\? &amp; \text{for } x=0\end{cases}\) 图形： 附录 Order of continuity(函数连续性) 函数参数连续性是个人的意译，大概意思应该是激活函数曲线的连续性类型。wiki中有对各个值的说明，可参考Order_of_continuity和描述光滑度中对于曲线、曲面的C0、C1、C2和G0、G1、G2定义和区别 - chenchen_fcj程序猿的博客 - CSDN博客，简单说明如下 \(C^{-1}\): 曲线不连续 \(C^{0}\): 曲线本身是连续的，但不可导 \(C^{1}\): 曲线一阶导是连续的，但无法二阶导 \(C^{2}\): 曲线一阶导和二阶导是连续的，但无法三阶导 \(C^{n}\): 曲线一阶导到n阶导是连续的，但无法进行n+1阶导 \(C^{\infty}\): 曲线无穷阶可导 参考 visualising-activation-functions-in-neural-networks Activation_function https://zhuanlan.zhihu.com/p/32824193]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数图形]]></title>
    <url>%2F2019%2F07%2F09%2Factivation_functions%2F</url>
    <content type="text"><![CDATA[#visualisation{height:500px}.axis line,.axis path{fill:none;stroke:#777;shape-rendering:crispEdges}.axis text{font-family:Arial;font-size:.6em}.tick{stroke-dasharray:1,2}.bar{fill:#b22222}.legend{cursor:pointer}.MathJax_SVG svg>g,.MathJax_SVG_Display svg>g{fill:#000;stroke:#000}.legend{font-size:1.2em}.graph_inputs{font-size:.8em}.func_text{font-size:.65em}.deriv_func_text{font-size:.65em}#select_activ{font-size:.8em}.input_labels{font-size:.8em}#func_equation{font:"Helvetica Neue";font-size:.9em}#deriv_func_equation{font:"Helvetica Neue";font-size:.9em}#rrelu_button{background-color:#090;color:#fff;text-align:center;text-decoration:none;display:inline-block;font-size:.8em;cursor:pointer;margin-left:.3em}#rrelu_button:hover{background-color:#4caf50;color:#fff} 版权所有为 David Sheehan 原blog地址为：Visualising Activation Functions in Neural NetworksNote: 注意：建议您在Chrome上查看以获得最佳体验。在Firefox和IE上，框中的等式可能无法呈现。StepIdentityReLuSigmoidTanhLeaky ReLUPReLURReLUELUSELUSReLUHard SigmoidHard TanhLeCun TanhArcTanSoftSignSoftplusSignumBent IdentitySymmetrical SigmoidLog LogGaussianAbsoluteSinusoidCosSincα αr tl tr Random α这种激活函数更具理论性而非实际性，模仿生物神经元的全有或全无特性。它对神经网络没有 用，因为它的导数是零（除了0，它是未定义的）。这意味着基于梯度的优化方法是不可行的。MathJax.Hub.Config({ tex2jax: { inlineMath: [["$", "$"], ["\\(", "\\)"]], processEscapes: true } }); function erf(x) { var z; const ERF_A = 0.147; var the_sign_of_x; if (0 == x) { the_sign_of_x = 0; return 0; } else if (x > 0) { the_sign_of_x = 1; } else { the_sign_of_x = -1; } var one_plus_axsqrd = 1 + ERF_A * x * x; var four_ovr_pi_etc = 4 / Math.PI + ERF_A * x * x; var ratio = four_ovr_pi_etc / one_plus_axsqrd; ratio *= x * -x; var expofun = Math.exp(ratio); var radical = Math.sqrt(1 - expofun); z = radical * the_sign_of_x; return z; } // https://stackoverflow.com/questions/12556685/is-there-a-javascript-implementation-of-the-inverse-error-function-akin-to-matl function erfINV(inputX) { var _a = (8 * (Math.PI - 3)) / (3 * Math.PI * (4 - Math.PI)); var _x = parseFloat(inputX); var signX = _x < 0 ? -1.0 : 1.0; var oneMinusXsquared = 1.0 - _x * _x; var LNof1minusXsqrd = Math.log(oneMinusXsquared); var PI_times_a = Math.PI * _a; var firstTerm = Math.pow(2.0 / PI_times_a + LNof1minusXsqrd / 2.0, 2); var secondTerm = LNof1minusXsqrd / _a; var thirdTerm = 2 / PI_times_a + LNof1minusXsqrd / 2.0; var primaryComp = Math.sqrt( Math.sqrt(firstTerm - secondTerm) - thirdTerm ); var scaled_R = signX * primaryComp; return scaled_R; } var delay = 200; var activ_func = document.getElementById("select_activ").value; var lineData = []; var lineData_deriv = []; func_tags = [ { func_tex: "$f(x) =\\begin{cases}1 & \\text{for } x\\geq0\\\\0 & \\text{for } x]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建blog时遇到的一些问题记录]]></title>
    <url>%2F2019%2F07%2F08%2Fproblems-during-create-blog%2F</url>
    <content type="text"><![CDATA[hexo 中文操作文档 这个网站上内容有点旧了 中文官网 next 中文官网 英文官网 搭建优化参考 可以照着官网做，也可以看 搭建自己的个人博客 或 hexo blog 美化升级 或 使用Hexo搭建博客之优化篇 关于评论系统 gitmeet 目前因为原作者的服务停了，放弃了 gitalk 可以用，如遇到Error，参考Hexo Next主题集成Gitalk 用git的issue做评论需注意创建的博客文档名长度要控制在150以内，用中文的话，因为会用url编码，所以很容易超，因此尽量用英文来命名文档 静态html，不希望转义 设置 layout: false ， 或者设置 skip_render lived看板娘 参考 hexo博客美化添加live2d hexo中next主题添加里lived看板娘 hexo 中文支持 如果 hexo 语言不生效，很可能是因为主题没有对应的语言配置文件，参考Hexo语言不生效问题 公式支持 next对mathjax提供两种源 cdn和mhchem。 cdn总是加载失败，用mhchem不错 在线聊天服务 Chatra 官网 dashboard 百度统计 网址 123#!/usr/bin/python3print("Hello, World!");]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
