<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[阅读论文方法汇总]]></title>
    <url>%2F2019%2F07%2F26%2F%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[三步法引自论文追求在三步内完成阅读。每一步都要完成一个明确目标，循序渐进。第一步要弄清一篇论文的大体想法；第二步要抓住论文的主要内容，但不是细节；第三步要深度理解整篇论文。 第一步目标：快速浏览全文，有大概了解。并决定是否进一步阅读。耗时：五到十分钟。细节： 认真阅读题目、摘要、引言。 阅读标题和段落开头，除此之外不看。 瞥一眼数学部分（如果有的话），可以掌握基本理论。 阅读结论部分 瞥一眼引用，找一下是否有读过的。 问题：完成第一步后，需回答以下几个问题。 类别：论文是什么类型？测试类？对现有系统分析？对原型系统描述？ 内容：读过的其他论文有没有与这个相关的，文章分析问题用到了什么理论基础？ 正确性：结论看起来是否真实有效？ 创新点：论文主要创新点是什么？ 清晰度：论文是否写的调理清晰？ 到此后大致能确定是否需要继续阅读。不在继续的原因可能是因为这篇论文不吸引你，或者你对这篇论文所涉及的领域了解不多，或者因为作者给出了不适当的假设。顺带一提，大部分审稿人或者读者往往只做第一步，所以在写论文时，标题、段落开头、摘要要精确且易于理解。可以多采用“图示摘要”。 第二步目标：仔细阅读论文，但证明等信息要忽略。在阅读时大概的记下重点，或在论文空白处标注。记录下不懂的部分或者想问作者的问题。耗时：依据论文大小而定，大概一小时。细节： 仔细阅读图片、表格和其他说明。确保图表正确，并理解图表含义。 记下重点 及时标注不懂的部分或想问作者的问题 大部分论文在完成第二步后应该有了一定理解，评判是否掌握的一个方法是尝试将论文的主要思想和相关论据讲给其他人听或者用简单的语句总结。如果第二步后还不能理解，可能是因为论文的主旨对你来说是全新的，对论文中的术语和缩写词陌生，也可能是作者用了你不理解的论据和试验方法，导致论文中大部分内容不好理解。也有可能论文写的较差，充满了未经证实的断言和大量引用。或者是因为你当前状态不好，面对这种情况，可以先了解相关背景知识后再阅读。 第三步目标： 彻底理解一篇论文。试着“在脑中重新实现”，即和作者一样做出相同假设，然后重新实现相同的工作。通过对比你的结果和论文给出的结果，可以验证一篇论文的创新点是否真实，还能发现论文中不会讲的缺点和假想。 大部分论文不需要做第三步，除非有需要。三步法是 S. Keshav 教授在07年发表的，S. Keshav是斯坦福大学教授，谷歌的一个投资人，曾是加拿大最富有的20位富豪之一。 如何阅读如何读各个部分参考博文 如何读标题：看完标题想想要是让你写你怎么用一句话表达这个标题。根据标题推测作者论文可能的内容。 如何读摘要：快速浏览一遍，看不懂没关系，继续往下看，看完后也许就懂了。因为摘要写的很简洁，省略了很多前提和条件。 如何读引言（前言）：如果对领域有一定了解，看引言就容易多了。引言一般是介绍性的内容，同领域内论文写的应该差不多，看多了后就很快了。有些写的好的可以记录，没准有用。 如何读材料及试验：无非是介绍实验方法，自己怎么做实验的。了解了这些就行。 如何看实验结果：一定要结合图和表看。主要看懂实验的结果，体会作者的表达方法（例如作者用不同的句子结构描述一些数字和结果）。 如何看分析与讨论：这是一篇文章的重点，最花时间。看之前可以先想想如果我做出这样的结果，我会怎样写分析和讨论，然后再慢慢看作者的分析和讨论。 如何看结论：细看分析和讨论后，结论就好看了，然后可以返回去看看摘要。 看后的论文及时分类整理，保存好相关笔记，笔记要做到再次看时，只需五分钟内就能完全理解。 看过的论文，不定时去温习下。 笔记主要记录，所在领域，所用方法，重要的结论，经典的句子，精巧的实验方案 对一些相似的文献，可以做横向比较，用批判性思路阅读。 批判式阅读 如果论点成立，论文内部是否一致，论据或者证据是否能支持论点 比较论述该论点的其他论文，看结论是否一致或者相反 查看引用文献，看论文如何适应在范围更大的语境。 提高阅读效率 集中时间看文献，更容易联系起来，形成整体印象。 做好记录和标记，直接在文献中标注，并做个汇总表，方便快速查阅 阅读顺序：一般是abstract, introduction, discussion, 然后是result, method(结合图表) 如何做笔记读书笔记要求Topic(重要吗？)，Theory(充分吗？)，Issue(有趣吗？有意义吗？)，Hypothesis(合理吗？)，Study(科学吗？)，Result(令人惊奇吗？)，Explanation(有价值/不足吗？)，Implication(有实用/收获吗？)，idea(你怎么做？) 读书笔记写法 在文章题目的空白处写一段总结（研究主题、亮点等） 文献旁边的空白可以做一些简单批注。 用一个专门的记录本记下文献中对自己有用的东西，可以考虑制定一个模板文件。 文献中不足的地方都应该记下来。 一定要记录清楚引文时必要的信息（题目、作者等） 对文献做分类，主题、是否精读等 重点分析或精读几篇代表性文献 重点写心得体会 重要数据及时记录下来 读书笔记模板 所在领域（主题） 作者简介（可选） 参考价值，可以作为温习的依据 心得体会 主要研究的课题 采用的方法 实验设计及思考 重要的结论 经典的句子 相似文献（可选） 论文信息（可选，需要引录时能找到）]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广告算法思考]]></title>
    <url>%2F2019%2F07%2F25%2F%E5%B9%BF%E5%91%8A%E7%AE%97%E6%B3%95%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[# 目的 1. 阐述下自己对广告投放算法的理解 2. 尝试用一个量化投资算法来做广告投放 个人的广告投放算法理解广告投放算法可以对比投资分析算法。 算法目的对比广告投放算法本质是帮忙平台方通过将有限的用户流量，合理的分配给广告主，在满足广告主投放成本需求的情况下，尽可能的提升平台方投放收益。投资算法有可以理解为投资者用有限的资本构建投资组合，买入投资标的，实现投资风险尽可能小的同时，获得超额收益。 名词映射如果屡一下映射关系的话 广告领域 投资领域 广告主 投资标的 用户 资本 平台方 投资者 选择用户曝光给某个广告主 选取资本买入某个标的 用户流量分配 构建投资组合 广告主投放成本 投资标的的投资风险 平台方投放收益 投资组合的超额收益 完全合理合法，有木有。 扩散思路既然两个领域这么相似，是不是可以尝试将投资算法用在广告投放上？不妨一试，不过需要注意广告主投资成本和投资风险之间的差异。原则上说，每个广告主平台方都要争取，即使这个广告主在成本上不具备优势。并且为了保证广告主的利益，在广告主的投放成本上要求相对严格，同时还要保证广告主的消耗。也就是说广告投放算法和投资组合算法在风险定义上还是存在很大差异的。另外，“用户”和“资本”之间也不完全一致，用户会随机出现在任意一个时刻，或者就不出现。并且因为广告主投放的预算存在上限，导致无法通过集中投放于某个广告主来达成平台目的。也就是说用户的出现是随机的，在做流量分配时，需要依据实时的用户分布和广告主实时消耗情况来做适当调整。 差异对比 广告领域 投资领域 每个广告主都要有 可以放弃某些标的 必须控制投放成本 可以依据风险收益比做抉择 广告主存在预算上限 可以选择上限更广的标的 必须满足广告主消耗 可以集中投放某些标的而不管其他的 用户随机分布，被动分配 可以控制资本，主动分配 看起来广告投放算法与投资算法的差异主要体现在各种限定条件上。这些限定条件增加了问题的复杂度，很难用一个简单模型进行拟合。每个限定条件就像是一个评审员，评判模型是否满足需求，并且它们的要求各有差异。其中最麻烦的就是用户随机分布，这要求模型能对未来用户分布有一定预期，使得不至于无法达到各个审判员（广告主）的要求。 算法探索如果将平台视为一个调度中心，那么每个广告主的投放可视为一个个体，这个个体能提供平台需要的收益，并且有自己对平台的要求。平台可以通过调度用户来满足各个个体的需求，同时尽可能提升自己的收益。 ToDo: 这里需要找一个投资模型作为对比，来设想一个模型。]]></content>
      <categories>
        <category>广告算法</category>
      </categories>
      <tags>
        <tag>广告算法</tag>
        <tag>待完善</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo中预览pdf]]></title>
    <url>%2F2019%2F07%2F11%2Fhexo-pdf%2F</url>
    <content type="text"><![CDATA[需求希望在博客中能预览pdf，hexo已有插件支持pdf预览，非常方便，只需安装插件和添加链接即可。 实现安装插件 1npm install --save hexo-pdf 使用在arkdown文件中添加pdf本地资源 1&#123;% pdf test.pdf %&#125; 在线资源 12&lt;center&gt; **监督学习**&lt;/center&gt; # 居中&#123;% pdf https://raw.githubusercontent.com/afshinea/stanford-cs-229-machine-learning/4d5ac4e620be8b63b6f889ecff667b7ebac008a8/zh/cheatsheet-supervised-learning.pdf %&#125; 问题处理github上pdf的显示要想成功显示pdf，必须正确配置路径，可以用F12来查看。若想显示github上的pdf，用github.com的域名是不行的，应该是raw.githubusercontent.com的域名，可以通过F12来查看。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习资料]]></title>
    <url>%2F2019%2F07%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[说明本文梳理下个人收集到的一些机器学习相关的资料。 VIP cheatsheets for Machine LearningAfshine Amidi (Ecole Centrale Paris, MIT) 和 Shervine Amidi (Ecole Centrale Paris, Stanford University) 两位大神开源的关于机器学习的VIP cheatsheets。意在通过一些小纸条就能记录下Standford’s CS 229机器学习课程上所有知识点，包括‘VIP Cheatshees’和‘VIP Refreshers’。 VIP Cheatsheets 是对机器学习课程上知识点的梳理，包括监督学习、无监督学习、深度学习、机器学习技巧和秘诀。 VIP Refreshers 是与机器学习相关基础知识的梳理，包括概率和统计、线性代数和微积分。 一份不错的纲领性资料，简单明了，已在github上开源，并且还有大神帮忙做了中文翻译，见stanford-cs-229-machine-learning。作者今年还写了stanford-cs-230-deep-learning和stanford-cs-221-artificial-intelligence，不过还没有中文翻译。下面罗列下机器学习课程相关cheatsheets的中文翻译文档。 监督学习 **监督学习** 无监督学习 **无监督学习** 深度学习 **深度学习** 机器学习技巧和秘诀 **机器学习技巧和秘诀** 概率和统计 **概率和统计** 线性代数和微积分 **线性代数和微积分**]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>资料</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的激活函数]]></title>
    <url>%2F2019%2F07%2F09%2Factivation_functions_in_deeplearning%2F</url>
    <content type="text"><![CDATA[说明本文对深度学习中的重要组件——激活函数做系统性汇总。 了解激活函数什么是激活函数在神经网络中，一个节点的激活函数(Activation Function)定义了该节点在给定的输入变量或输入集合下的输出。wiki中以计算机芯片电路为例，标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。激活函数主要用于提升神经网络解决非线性问题的能力。激活函数各式各样，各有优缺点，目前常用的有 ReLU、sigmoid、tanh等。各个激活函数的细节详见下文。 为什么需要激活函数当不用激活函数时，神经网络的权重和偏差只会进行线性变换。线性方程很简单，但是解决复杂问题的能力有限。没有激活函数的神经网络实质上就是一个线性回归模型。为了方便理解，以一个简单的例子来说明。考虑如下网络在不用激活函数的情况下，该图可用如下公式表示 $$output = w7(input1w1 +input2w2)+w8(input1w3+input2w4)+w9(input1w5+input2w6)$$ 实质是$output=\begin{bmatrix} w1w7+w3w8+w5w9 \ w2w7+w4w8+w6w9 \end{bmatrix} * \begin{bmatrix}input1 \ input2 \end{bmatrix} \implies Y=WX$的线性方程。 若在隐藏层引入激活函数$h(y)$，例如令$h(y) = max(y,0)$，那么原始式子就无法用简单线性方程表示了。 $$output=w7max(input1w1 +input2w2,0)+w8max(input1w3+input2w4,0)+w9max(input1w5+input2*w6,0)$$ 激活函数类型wiki中给出了激活函数的类型，这里略做归纳 Identity function (恒等函数) Step function (阶跃函数) Sigmoidal function (S形函数) Ramp function (斜坡函数) Binary 和 Bipolar 的区别应该是 Binary（单位）对应值为 0 或 1， Bipolar（两极）对应值为 -1 或 1 激活函数的一些特性 非线性(Nonlinear) 当激活函数是非线性的，那么一个两层神经网络也证明是一个通用近似函数通用近似理论。而恒等激活函数则无法满足这一特性，当多层网络的每一层都是恒等激活函数时，该网络实质等同于一个单层网络。 输出值域(Range) 若激活函数的值域是有限的，因为对权重的更新影响有限，所以基于梯度的训练方法更稳定。若值域是无限的，因为大多数情况下权重更新更明显，所以训练通常更有效，通常需要设定较小的学习率。 连续可微(Continuously differentiable) 通常情况下，当激活函数连续可微，则可以用基于梯度的优化方法。（也有例外，如ReLU函数虽不是连续可微，使用梯度优化也存在一些问题，如ReLU存在由于梯度过大或学习率过大而导致某个神经元输出小于0，从而使得该神经元输出始终是0，并且无法对与其相连的神经元参数进行更新，相当于该神经元进入了“休眠”状态，参考深度学习中，使用relu存在梯度过大导致神经元“死亡”，怎么理解？ - 知乎，但ReLU还是可以使用梯度优化的。）二值阶跃函数在0处不可微，并且在其他地方的导数是零，所以梯度优化方法不适用于该激活函数。 单调(Monotonic) 当激活函数为单调函数时，单层模型的误差曲面一定是凸面。即对应的误差函数是凸函数，求得的最小值一定是全局最小值。 一阶导单调(Smooth functions with a monotonic derivative) 通常情况下，这些函数表现更好。 原点近似恒等函数(Approximates identity near the origin) 若激活函数有这一特性，神经网络在随机初始化较小的权重时学习更高效。若激活函数不具备这一特性，初始化权重时必须特别小心。参考machine learning - Why activation functions that approximate the identity near origin are preferable? - Cross Validated 激活函数速查表以下是输入为一个变量的激活函数列表，主要参考自 wiki 函数 图形 值域 连续性 单调 一阶导单调 原点近似恒等 Identity(恒等函数) ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{\infty }}$ 是 是 是 Binary step(单位阶跃函数) ${\displaystyle {0,1}}$ ${\displaystyle C^{-1}}$ 是 否 否 Sigmoid(S函数又称Logistic逻辑函数) ${\displaystyle (0,1)}$ ${\displaystyle C^{\infty }}$ 是 否 否 TanH(双曲正切函数) ${\displaystyle (-1,1)}$ ${\displaystyle C^{\infty }}$ 是 否 是 ArcTan(反正切函数) ${\displaystyle \left(-{\frac {\pi }{2}},{\frac {\pi }{2}}\right)}$ ${\displaystyle C^{\infty }}$ 是 否 是 Softsign函数 ${\displaystyle (-1,1)}$ ${\displaystyle C^{1}}$ 是 否 是 Inverse square root unit(反平方根函数,ISRU) ${\displaystyle \left(-{\frac {1}{\sqrt {\alpha }}},{\frac {1}{\sqrt {\alpha }}}\right)}$ ${\displaystyle C^{\infty }}$ 是 否 是 Inverse square root linear unit(反平方根线性函数,ISRLU) ${\displaystyle \left(-{\frac {1}{\sqrt {\alpha }}},\infty \right)}$ ${\displaystyle C^{2}}$ 是 是 是 Square Nonlinearity(平方非线性函数,SQNL) ${\displaystyle (-1,1)}$ ${\displaystyle C^{2}}$ 是 否 是 Rectified linear unit(线性整流函数,ReLU) ${\displaystyle [0,\infty )}$ ${\displaystyle C^{0}}$ 是 是 否 Bipolar rectified linear unit(二级线性整流函数,BReLU) ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{0}}$ 是 是 否 Leaky rectified linear unit(带泄露随机线性整流函数,Leaky ReLU) ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{0}}$ 是 是 否 Parameteric rectified linear unit(参数化线性整流函数,PReLU) ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{0}}$ 是 iff ${\displaystyle \alpha \geq 0}$ 是 是 iff ${\displaystyle \alpha = 1}$ Randomized leaky rectified linear unit(带泄露随机线性整流函数,RReLU) ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{0}}$ 是 是 否 Exponential linear unit(指数线性函数,ELU) ${\displaystyle (-\alpha ,\infty )}$ ${\displaystyle {\begin{cases}C_{1}&amp;{\text{when }}\alpha =1\C_{0}&amp;{\text{otherwise }}\end{cases}}}$ 是 iff ${\displaystyle \alpha \geq 0}$ 是 iff ${\displaystyle 0\leq \alpha \leq 1}$ 是 iff ${\displaystyle \alpha = 1}$ Scaled exponential linear unit(扩展指数线性函数,SELU) ${\displaystyle (-\lambda \alpha ,\infty )}$ ${\displaystyle C^{0}}$ 是 否 否 S-shaped rectified linear activation unit(S型线性整流激活函数,SReLU) ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{0}}$ 否 否 否 Adaptive piecewise linear(自适应分段线性函数,APL) ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{0}}$ 否 否 否 SoftPlus函数 ${\displaystyle (0,\infty )}$ ${\displaystyle C^{\infty }}$ 是 是 否 Bent identity(弯曲恒等函数) ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{\infty }}$ 是 是 是 Sigmoid-weighted linear unit (SiLU)[11] (也被称为Swish[12]) ${\displaystyle [\approx -0.28,\infty )}$ ${\displaystyle C^{\infty }}$ 否 否 否 SoftExponential函数 ${\displaystyle (-\infty ,\infty )}$ ${\displaystyle C^{\infty }}$ 是 是 是 iff ${\displaystyle \alpha = 0}$ Soft Clipping(柔性剪峰函数) ${\displaystyle (0,1)}$ ${\displaystyle C^{\infty }}$ 是 否 否 Sinusoid正弦函数) ${\displaystyle [-1,1]}$ ${\displaystyle C^{\infty }}$ 否 否 是 Sinc函数 ${\displaystyle [\approx -.217234,1]}$ ${\displaystyle C^{\infty }}$ 否 否 否 Gaussian(高斯函数) ${\displaystyle (0,1]}$ ${\displaystyle C^{\infty }}$ 否 否 否 Hard Sigmoid(分段近似Sigmoid函数) $[0,1]$ $C^{0}$ 是 否 否 Hard Tanh(分段近似Tanh函数) $[-1,1]$ $C^{0}$ 是 否 是 LeCun Tanh(也称Scaled Tanh,按比例缩放的Tanh函数) $(-1.7519 , 1.7519)$ $C^{\infty }$ 是 否 否 Symmetrical Sigmoid(对称Sigmoid函数) $(-1, -1)$ $C^{\infty }$ 是 否 否 Complementary Log Log函数 $(0, 1)$ $C^{\infty }$ 是 否 否 Absolute(绝对值函数) $[0, \infty)$ $C^{0 }$ 否 否 否 激活函数详细描述函数图形说明函数图形主要取自Visualising Activation Functions in Neural Networks。翻译版本访问这里左侧蓝线是激活函数方程式图形，黄色是激活函数一阶导图形。右侧篮框是对激活函数方程式的各项特性描述，包括： Range(激活函数输出值域) Monotonic(激活函数是否单调) Continuity(激活函数连续性类型) Identity at Origin(激活函数在原点处是否近似恒等) Symmetry(激活函数是否对称) 右侧黄框是对激活函数一阶导的各项特性描述，包括： Range(一阶导输出值域) Monotonic(一阶导是否单调) Continuous(一阶导是否连续) Vanishing Gradient(是否梯度消失:指随着网络向后传递，是否存在当梯度值过小时，梯度变化以指数形式衰减，直至消失，导致后面的网络节点几乎无法更新参数) Exploding Gradient(是否梯度爆炸:指是否存在当梯度值过大，梯度变化以指数形式增加，直至爆炸) Saturation(饱和:指激活函数值接近其边界时，如sigmoid函数值接近0或1时，函数曲线是否平缓，过于平缓可能会导致在向后传播时梯度消失) Dead Neurons(神经元死亡:指在网络传播时，是否会出现某个神经元永远不会再更新的情况) Identity(恒等函数)描述： 一种输入和输出相等的激活函数，比较适合线性问题，如线性回归问题。但不适用于解决非线性问题。方程式： ${\displaystyle f(x)=x}$一阶导： ${\displaystyle f’(x)=1}$图形： Binary step(单位阶跃函数)描述： step与神经元激活的含义最贴近，指当刺激超过阈值时才会激发。但是由于该函数的梯度始终为0，不能作为深度网络的激活函数方程式： ${\displaystyle f(x)={\begin{cases}0&amp;{\text{for }}x&lt;0\1&amp;{\text{for }}x\geq 0\end{cases}}}$一阶导： ${\displaystyle f^{\prime}(x)={\begin{cases}0&amp;{\text{for }}x\neq 0\?&amp;{\text{for }}x=0\end{cases}}}$图形： Sigmoid(S函数又称Logistic逻辑函数)描述： 使用很广的一类激活函数，具有指数函数形状，在物理意义上最接近生物神经元。并且值域在(0,1)之间，可以作为概率表示。该函数也通常用于对输入的归一化，如Sigmoid交叉熵损失函数。Sigmoid激活函数具有梯度消失和饱和的问题，一般来说，sigmoid网络在5层之内就会产生梯度消失现象。方程式： ${\displaystyle f(x)=\sigma (x)={\frac {1}{1+e^{-x}}}}$一阶导： ${\displaystyle f’(x)=f(x)(1-f(x))}$图形： TanH(双曲正切函数)描述： TanH与Sigmoid函数类似，在输入很大或很小时，输出几乎平滑，梯度很小，不利于权重更新，容易出现梯度消失和饱和的问题。不过TanH函数值域在(-1,1)之间，以0为中心反对称，且原点近似恒等，这些点是加分项。一般二分类问题中，隐藏层用tanh函数，输出层用sigmod函数。方程式： ${\displaystyle f(x)=\tanh(x)={\frac {(e^{x}-e^{-x})}{(e^{x}+e^{-x})}}}$一阶导： ${\displaystyle f’(x)=1-f(x)^{2}}$图形： ArcTan(反正切函数)描述： ArcTen从图形上看类似TanH函数，只是比TanH平缓，值域更大。从一阶导看出导数趋于零的速度比较慢，因此训练比较快。方程式： ${\displaystyle f(x)=\tan ^{-1}(x)}$一阶导： ${\displaystyle f’(x)={\frac {1}{x^{2}+1}}}$图形： Softsign函数描述： Softsign从图形上看也类似TanH函数，以0为中心反对称，训练比较快。方程式： ${\displaystyle f(x)={\frac {x}{1+|x|}}}$一阶导： ${\displaystyle f’(x)={\frac {1}{(1+|x|)^{2}}}}$图形： Inverse square root unit(反平方根函数,ISRU)描述： 图形类似于tanH和Sigmoid函数，论文说可作为RNN层的激活函数，号称在RNN层中达到与tanh和sigmoid一样效果的情况下，计算复杂度更低。方程式： ${\displaystyle f(x)={\frac {x}{\sqrt {1+\alpha x^{2}}}}}$一阶导： ${\displaystyle f’(x)=\left({\frac {1}{\sqrt {1+\alpha x^{2}}}}\right)^{3}}$图形： Inverse square root linear unit(反平方根线性函数,ISRLU)描述： 一个分段函数，小于0部分是ISRU，大于等于0部分是恒等函数，论文里说这函数在CNN层上相较于ReLU学习更快，更一般化。方程式： ${\displaystyle f(x)={\begin{cases}{\frac {x}{\sqrt {1+\alpha x^{2}}}}&amp;{\text{for }}x&lt;0\x&amp;{\text{for }}x\geq 0\end{cases}}}$一阶导： ${\displaystyle f’(x)={\begin{cases}\left({\frac {1}{\sqrt {1+\alpha x^{2}}}}\right)^{3}&amp;{\text{for }}x&lt;0\1&amp;{\text{for }}x\geq 0\end{cases}}}$图形： Square Nonlinearity(平方非线性函数,SQNL)描述： 类似Softsign的函数，论文摘要中将这个与Softsign对比，说该函数在收敛速度上更快。方程式： ${\displaystyle f(x)={\begin{cases}1&amp;:x&gt;2.0\x-{\frac {x^{2}}{4}}&amp;:0\leq x\leq 2.0\x+{\frac {x^{2}}{4}}&amp;:-2.0\leq x&lt;0\-1&amp;:x&lt;-2.0\end{cases}}}$一阶导： ${\displaystyle f’(x)=1\mp {\frac {x}{2}}}$图形： Rectified linear unit(线性整流函数,ReLU)描述： 比较流行的激活函数，该函数保留了类似step那样的生物学神经元机制，即高于0才激活，不过因在0以下的导数都是0，可能会引起学习缓慢甚至神经元死亡的情况。方程式： ${\displaystyle f(x)={\begin{cases}0&amp;{\text{for }}x\leq 0\x&amp;{\text{for }}x&gt;0\end{cases}}}$一阶导： ${\displaystyle f’(x)={\begin{cases}0&amp;{\text{for }}x\leq 0\1&amp;{\text{for }}x&gt;0\end{cases}}}$图形： Bipolar rectified linear unit(二级线性整流函数,BReLU)描述： relu系列的一种激活函数，形式很奇怪，采用mod 2作为条件分段，论文说在个别场景上能取得更好的效果。方程式： $f(x_{i})={\begin{cases}ReLU(x_{i})&amp;{\text{if }}i{\bmod {2}}=0\-ReLU(-x_{i})&amp;{\text{if }}i{\bmod {2}}\neq 0\end{cases}}$一阶导： ${\displaystyle f’(x_{i})={\begin{cases}ReLU’(x_{i})&amp;{\text{if }}i{\bmod {2}}=0\-ReLU’(-x_{i})&amp;{\text{if }}i{\bmod {2}}\neq 0\end{cases}}}$图形： Leaky rectified linear unit(带泄露随机线性整流函数,Leaky ReLU)描述： relu的一个变化，即在小于0部分不等于0，而是加一个很小的不为零的斜率，减少神经元死亡带来的影响。方程式： ${ f(x)={\begin{cases}0.01x&amp;{\text{for }}x&lt;0\x&amp;{\text{for }}x\geq 0\end{cases}}}$一阶导： ${\displaystyle f’(x)={\begin{cases}0.01&amp;{\text{for }}x&lt;0\1&amp;{\text{for }}x\geq 0\end{cases}}}$图形： Parameteric rectified linear unit(参数化线性整流函数,PReLU)描述： 也是ReLU的一个变化，与Leaky ReLU类似，只不过PReLU将小于零部分的斜率换成了可变参数α。这种变化使值域会依据α不同而不同。方程式： ${\displaystyle f(\alpha ,x)={\begin{cases}\alpha x&amp;{\text{for }}x&lt;0\x&amp;{\text{for }}x\geq 0\end{cases}}}$一阶导： ${ f’(\alpha ,x)={\begin{cases}\alpha &amp;{\text{for }}x&lt;0\1&amp;{\text{for }}x\geq 0\end{cases}}}$图形： Randomized leaky rectified linear unit(带泄露随机线性整流函数,RReLU)描述： 在PReLU基础上将α变成了随机数。参考论文方程式： ${\displaystyle f(\alpha ,x)={\begin{cases}\alpha x&amp;{\text{for }}x&lt;0\x&amp;{\text{for }}x\geq 0\end{cases}}}$一阶导： ${ f’(\alpha ,x)={\begin{cases}\alpha &amp;{\text{for }}x&lt;0\1&amp;{\text{for }}x\geq 0\end{cases}}}$图形： Exponential linear unit(指数线性函数,ELU)描述： ELU小于零的部分采用了负指数形式，相较于ReLU权重可以有负值，并且在输入取较小值时具有软饱和的特性，提升了对噪声的鲁棒性。参考ELU激活函数的提出方程式： ${\displaystyle f(\alpha ,x)={\begin{cases}\alpha (e^{x}-1)&amp;{\text{for }}x\leq 0\x&amp;{\text{for }}x&gt;0\end{cases}}}$一阶导： ${ f’(\alpha ,x)={\begin{cases}f(\alpha ,x)+\alpha &amp;{\text{for }}x\leq 0\1&amp;{\text{for }}x&gt;0\end{cases}}}$图形： Scaled exponential linear unit(扩展指数线性函数,SELU)描述： ELU的一种变化，引入超参λ和α，并给出了相应取值，这些取值在原论文中（Self-Normalizing Neural Networks）详细推导过程方程式： ${ f(\alpha ,x)=\lambda {\begin{cases}\alpha (e^{x}-1)&amp;{\text{for }}x&lt;0\x&amp;{\text{for }}x\geq 0\end{cases}}} $$with \quad{\displaystyle \lambda =1.0507} \quad and \quad {\displaystyle \alpha =1.67326}$一阶导： ${\displaystyle f’(\alpha ,x)=\lambda {\begin{cases}\alpha (e^{x})&amp;{\text{for }}x&lt;0\1&amp;{\text{for }}x\geq 0\end{cases}}}$图形： S-shaped rectified linear activation unit(S型线性整流激活函数,SReLU)描述： 也是ReLU的一种变化，不同的是该函数有三个分段，四个超参数，这种设置使函数图形看起来像S型，具体参考论文。方程式： ${\displaystyle f_{t_{l},a_{l},t_{r},a_{r}}(x)={\begin{cases}t_{l}+a_{l}(x-t_{l})&amp;{\text{for }}x\leq t_{l}\x&amp;{\text{for }}t_{l}&lt;x&lt;t_{r}\t_{r}+a_{r}(x-t_{r})&amp;{\text{for }}x\geq t_{r}\end{cases}}}$一阶导： ${\displaystyle f’{t{l},a_{l},t_{r},a_{r}}(x)={\begin{cases}a_{l}&amp;{\text{for }}x\leq t_{l}\1&amp;{\text{for }}t_{l}&lt;x&lt;t_{r}\a_{r}&amp;{\text{for }}x\geq t_{r}\end{cases}}}$图形： Adaptive piecewise linear(自适应分段线性函数,APL)描述： 原论文表示通过分段为神经元学习加入自适应激活功能，比ReLU在cifar-10、cifar-100上有更好的性能。方程式： ${\displaystyle f(x)=\max(0,x)+\sum {s=1}^{S}a{i}^{s}\max(0,-x+b_{i}^{s})}$一阶导： ${\displaystyle f’(x)=H(x)-\sum {s=1}^{S}a{i}^{s}H(-x+b_{i}^{s})}$图形： SoftPlus函数描述： 是ReLU的平滑替代，函数在任何地方连续且值域非零，避免了死神经元。不过因不对称且不以零为中心，可以影响网络学习。由于导数必然小于1，所以也存在梯度消失问题。方程式： ${\displaystyle f(x)=\ln(1+e^{x})}$一阶导： ${\displaystyle f’(x)={\frac {1}{1+e^{-x}}}}$图形： Bent identity(弯曲恒等函数)描述： 可以理解为identity和ReLU之间的一种折中，不会出现死神经元的问题，不过存在梯度消失和梯度爆炸风险。方程式： $\begin{align}f(x)=\frac{\sqrt{x^2 + 1} - 1}{2} + x \end{align}$一阶导： ${\displaystyle f’(x)={\frac {x}{2{\sqrt {x^{2}+1}}}}+1}$图形： Sigmoid-weighted linear unit (SiLU)描述： 具体参考论文方程式： ${\displaystyle f(x)=x\cdot \sigma (x)}$一阶导： ${\displaystyle f’(x)=f(x)+\sigma (x)(1-f(x))}$图形： SoftExponential函数描述： 参考论文方程式： ${\displaystyle f(\alpha ,x)={\begin{cases}-{\frac {\ln(1-\alpha (x+\alpha ))}{\alpha }}&amp;{\text{for }}\alpha &lt;0\x&amp;{\text{for }}\alpha =0\{\frac {e^{\alpha x}-1}{\alpha }}+\alpha &amp;{\text{for }}\alpha &gt;0\end{cases}}}$一阶导： ${\displaystyle f’(\alpha ,x)={\begin{cases}{\frac {1}{1-\alpha (\alpha +x)}}&amp;{\text{for }}\alpha &lt;0\e^{\alpha x}&amp;{\text{for }}\alpha \geq 0\end{cases}}}$图形： Soft Clipping(柔性剪峰函数)描述： 参考论文方程式： ${\displaystyle f(\alpha ,x)={\frac {1}{\alpha }}\log {\frac {1+e^{\alpha x}}{1+e^{\alpha (x-1)}}}}$一阶导： ${\displaystyle f’(\alpha ,x)={\frac {1}{2}}\sinh \left({\frac {p}{2}}\right){\text{sech}}\left({\frac {px}{2}}\right){\text{sech}}\left({\frac {p}{2}}(1-x)\right)}$图形： Sinusoid正弦函数)描述： Sinusoid作为激活函数，为神经网络引入了周期性，且该函数处处联系，以零点对称。参考论文方程式： ${\displaystyle f(x)=\sin(x)}$一阶导： ${\displaystyle f’(x)=\cos(x)}$图形： Sinc函数描述： Sinc函数在信号处理中尤为重要，因为它表征了矩形函数的傅立叶变换。作为激活函数，它的优势在于处处可微和对称的特性，不过容易产生梯度消失的问题。方程式： ${\displaystyle f(x)={\begin{cases}1&amp;{\text{for }}x=0\{\frac {\sin(x)}{x}}&amp;{\text{for }}x\neq 0\end{cases}}}$一阶导： ${\displaystyle f’(x)={\begin{cases}0&amp;{\text{for }}x=0\{\frac {\cos(x)}{x}}-{\frac {\sin(x)}{x^{2}}}&amp;{\text{for }}x\neq 0\end{cases}}}$图形： Gaussian(高斯函数)描述： 高斯激活函数不常用。方程式： ${\displaystyle f(x)=e^{-x^{2}}}$一阶导： ${\displaystyle f’(x)=-2xe^{-x^{2}}}$图形： Hard Sigmoid(分段近似Sigmoid函数)描述： 是Sigmoid函数的分段线性近似，更容易计算，不过存在梯度消失和神经元死亡的问题方程式： $f(x) =\begin{cases}0 &amp; \text{for } x&lt;-2.5\0.2x + 0.5 &amp; \text{for } -2.5\geq x\leq 2.5 \1 &amp; \text{for } x&gt;2.5\end{cases}$一阶导： $f’(x) =\begin{cases}0 &amp; \text{for } x&lt;-2.5\0.2 &amp; \text{for } -2.5\geq x\leq 2.5 \0 &amp; \text{for } x&gt;2.5\end{cases}$图形： Hard Tanh(分段近似Tanh函数)描述： Tanh激活函数的分段线性近似。方程式： $f(x) =\begin{cases}-1 &amp; \text{for } x&lt;-1\x &amp; \text{for } -1\geq x\leq 1 \1 &amp; \text{for } x&gt;1 \end{cases}$一阶导： $f’(x) =\begin{cases}0 &amp; \text{for } x&lt;-1\1 &amp; \text{for } -1\geq x\leq 1 \0 &amp; \text{for } x&gt;1\end{cases}$图形： LeCun Tanh(也称Scaled Tanh,按比例缩放的Tanh函数)描述： Tanh的缩放版本，参考论文方程式： $\begin{align}f(x)&amp;=1.7519\tanh(\frac{2}{3}x)\end{align}$一阶导： $\begin{align}f’(x)&amp;=1.7519\frac{2}{3}(1-\tanh^2(\frac{2}{3}x))\&amp;=1.7519\frac{2}{3}-\frac{2}{3*1.7519}f(x)^2\end{align}$图形： Symmetrical Sigmoid(对称Sigmoid函数)描述： 是Tanh的一种替代方法，比Tanh形状更扁平，导数更小，下降更缓慢。方程式： $\begin{align}f(x)&amp;=\tanh(x/2)\&amp;=\frac{1-e^{-x}}{1+e^{-x}}\end{align}$一阶导： $\begin{align}f’(x)&amp;=0.5(1-\tanh^2(x/2))\&amp;=0.5(1-f(x)^2)\end{align}$图形： Complementary Log Log函数描述： 是Sigmoid的一种替代，相较于Sigmoid更饱和。方程式： $f(x)=1-e^{-e^{x}}$一阶导： $\begin{align}f’(x)=e^{x}(e^{-e^{x}}) = e^{x-e^{x}}\end{align}$图形： Absolute(绝对值函数)描述： 导数只有两个值。方程式： $\begin{align}f(x)=|x|\end{align}$一阶导： $f’(x) =\begin{cases}-1 &amp; \text{for } x&lt;0\1 &amp; \text{for } x&gt;0\? &amp; \text{for } x=0\end{cases}$图形： 附录Order of continuity(函数连续性)函数参数连续性是个人的意译，大概意思应该是激活函数曲线的连续性类型。wiki中有对各个值的说明，可参考Order_of_continuity和描述光滑度中对于曲线、曲面的C0、C1、C2和G0、G1、G2定义和区别 - chenchen_fcj程序猿的博客 - CSDN博客，简单说明如下 $C^{-1}$: 曲线不连续 $C^{0}$: 曲线本身是连续的，但不可导 $C^{1}$: 曲线一阶导是连续的，但无法二阶导 $C^{2}$: 曲线一阶导和二阶导是连续的，但无法三阶导 $C^{n}$: 曲线一阶导到n阶导是连续的，但无法进行n+1阶导 $C^{\infty}$: 曲线无穷阶可导 参考visualising-activation-functions-in-neural-networksActivation_functionhttps://zhuanlan.zhihu.com/p/32824193]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[激活函数图形]]></title>
    <url>%2F2019%2F07%2F09%2Factivation_functions%2F</url>
    <content type="text"><![CDATA[#visualisation{height:500px}.axis line,.axis path{fill:none;stroke:#777;shape-rendering:crispEdges}.axis text{font-family:Arial;font-size:.6em}.tick{stroke-dasharray:1,2}.bar{fill:#b22222}.legend{cursor:pointer}.MathJax_SVG svg>g,.MathJax_SVG_Display svg>g{fill:#000;stroke:#000}.legend{font-size:1.2em}.graph_inputs{font-size:.8em}.func_text{font-size:.65em}.deriv_func_text{font-size:.65em}#select_activ{font-size:.8em}.input_labels{font-size:.8em}#func_equation{font:"Helvetica Neue";font-size:.9em}#deriv_func_equation{font:"Helvetica Neue";font-size:.9em}#rrelu_button{background-color:#090;color:#fff;text-align:center;text-decoration:none;display:inline-block;font-size:.8em;cursor:pointer;margin-left:.3em}#rrelu_button:hover{background-color:#4caf50;color:#fff} 版权所有为 David Sheehan 原blog地址为：Visualising Activation Functions in Neural NetworksNote: 注意：建议您在Chrome上查看以获得最佳体验。在Firefox和IE上，框中的等式可能无法呈现。StepIdentityReLuSigmoidTanhLeaky ReLUPReLURReLUELUSELUSReLUHard SigmoidHard TanhLeCun TanhArcTanSoftSignSoftplusSignumBent IdentitySymmetrical SigmoidLog LogGaussianAbsoluteSinusoidCosSincα αr tl tr Random α这种激活函数更具理论性而非实际性，模仿生物神经元的全有或全无特性。它对神经网络没有 用，因为它的导数是零（除了0，它是未定义的）。这意味着基于梯度的优化方法是不可行的。MathJax.Hub.Config({ tex2jax: { inlineMath: [["$", "$"], ["\\(", "\\)"]], processEscapes: true } }); function erf(x) { var z; const ERF_A = 0.147; var the_sign_of_x; if (0 == x) { the_sign_of_x = 0; return 0; } else if (x > 0) { the_sign_of_x = 1; } else { the_sign_of_x = -1; } var one_plus_axsqrd = 1 + ERF_A * x * x; var four_ovr_pi_etc = 4 / Math.PI + ERF_A * x * x; var ratio = four_ovr_pi_etc / one_plus_axsqrd; ratio *= x * -x; var expofun = Math.exp(ratio); var radical = Math.sqrt(1 - expofun); z = radical * the_sign_of_x; return z; } // https://stackoverflow.com/questions/12556685/is-there-a-javascript-implementation-of-the-inverse-error-function-akin-to-matl function erfINV(inputX) { var _a = (8 * (Math.PI - 3)) / (3 * Math.PI * (4 - Math.PI)); var _x = parseFloat(inputX); var signX = _x < 0 ? -1.0 : 1.0; var oneMinusXsquared = 1.0 - _x * _x; var LNof1minusXsqrd = Math.log(oneMinusXsquared); var PI_times_a = Math.PI * _a; var firstTerm = Math.pow(2.0 / PI_times_a + LNof1minusXsqrd / 2.0, 2); var secondTerm = LNof1minusXsqrd / _a; var thirdTerm = 2 / PI_times_a + LNof1minusXsqrd / 2.0; var primaryComp = Math.sqrt( Math.sqrt(firstTerm - secondTerm) - thirdTerm ); var scaled_R = signX * primaryComp; return scaled_R; } var delay = 200; var activ_func = document.getElementById("select_activ").value; var lineData = []; var lineData_deriv = []; func_tags = [ { func_tex: "$f(x) =\\begin{cases}1 & \\text{for } x\\geq0\\\\0 & \\text{for } x]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建blog时遇到的一些问题记录]]></title>
    <url>%2F2019%2F07%2F08%2Fproblems-during-create-blog%2F</url>
    <content type="text"><![CDATA[hexo中文操作文档 这个网站上内容有点旧了中文官网 next中文官网 英文官网 搭建优化参考可以照着官网做，也可以看 搭建自己的个人博客 或 hexo blog 美化升级 或 使用Hexo搭建博客之优化篇 关于评论系统gitmeet 目前因为原作者的服务停了，放弃了gitalk 可以用，如遇到Error，参考Hexo Next主题集成Gitalk用git的issue做评论需注意创建的博客文档名长度要控制在150以内，用中文的话，因为会用url编码，所以很容易超，因此尽量用英文来命名文档 静态html，不希望转义设置 layout: false ， 或者设置 skip_render lived看板娘参考 hexo博客美化添加live2d hexo中next主题添加里lived看板娘 hexo 中文支持如果 hexo 语言不生效，很可能是因为主题没有对应的语言配置文件，参考Hexo语言不生效问题 公式支持next对mathjax提供两种源 cdn和mhchem。 cdn总是加载失败，用mhchem不错 在线聊天服务 Chatra官网 dashboard 百度统计网址 123#!/usr/bin/python3print("Hello, World!");]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
