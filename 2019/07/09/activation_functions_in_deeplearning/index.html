<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/upload/image/icon-32.png?v=7.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/upload/image/icon-16.png?v=7.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.2.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: true,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <meta name="description" content="说明 本文对深度学习中的重要组件——激活函数做系统性汇总。 了解激活函数 什么是激活函数 在神经网络中，一个节点的激活函数(Activation Function)定义了该节点在给定的输入变量或输入集合下的输出。wiki中以计算机芯片电路为例，标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。激活函数主要用于提升神经网络解决非线性问题的能力。激活函数各式各样，各有">
<meta name="keywords" content="深度学习,激活函数">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习的激活函数">
<meta property="og:url" content="https://agentai.github.io/2019/07/09/activation_functions_in_deeplearning/index.html">
<meta property="og:site_name" content="多巴胺授体">
<meta property="og:description" content="说明 本文对深度学习中的重要组件——激活函数做系统性汇总。 了解激活函数 什么是激活函数 在神经网络中，一个节点的激活函数(Activation Function)定义了该节点在给定的输入变量或输入集合下的输出。wiki中以计算机芯片电路为例，标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。激活函数主要用于提升神经网络解决非线性问题的能力。激活函数各式各样，各有">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-ebc14f6a033ca8a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-5710b0fd9c5ee858.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-b2b444d1e96bf8e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-7ecc2d07df55ae67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-9210b7ff830fdc94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-03c83295f756f2e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-1042e88c2c7c93ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-4bc4074325eb5c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-0e78f5b1f5f92c65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-245230edcd3a4a9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-aaaa71e376ca3391.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-c08bbbca6e05005b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-de814e275edfd1de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-193a91a0a6deee5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-7c8d441bd88e7593.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-20873dee2d68795d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-813a02444c461bf1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-6b723b46e8e6d58a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-e78b931a88ee79c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-d48fb0c9bcd7aeb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-1d977dd9ebda9431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-a090a7783d0d8779.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-d3e3e1eb128a9880.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-dd49869c7fea9686.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-b9849f4e71ae2f9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-2c3e53ea13dc080e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-b5d4e0248cb1acfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-1bb660c33c3059d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-5aaa82d797f96fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-08cdebbecc3d058b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-7d837d66e7bac174.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-5710b0fd9c5ee858.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-b2b444d1e96bf8e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-7ecc2d07df55ae67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-9210b7ff830fdc94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-03c83295f756f2e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-1042e88c2c7c93ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-4bc4074325eb5c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-0e78f5b1f5f92c65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-245230edcd3a4a9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-aaaa71e376ca3391.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-c08bbbca6e05005b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-de814e275edfd1de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-193a91a0a6deee5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-7c8d441bd88e7593.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-20873dee2d68795d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-813a02444c461bf1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-6b723b46e8e6d58a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-e78b931a88ee79c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-d48fb0c9bcd7aeb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-1d977dd9ebda9431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-a090a7783d0d8779.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-d3e3e1eb128a9880.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-dd49869c7fea9686.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-b9849f4e71ae2f9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-2c3e53ea13dc080e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-b5d4e0248cb1acfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-1bb660c33c3059d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-5aaa82d797f96fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-08cdebbecc3d058b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/18600497-7d837d66e7bac174.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-07-11T02:35:25.613Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习的激活函数">
<meta name="twitter:description" content="说明 本文对深度学习中的重要组件——激活函数做系统性汇总。 了解激活函数 什么是激活函数 在神经网络中，一个节点的激活函数(Activation Function)定义了该节点在给定的输入变量或输入集合下的输出。wiki中以计算机芯片电路为例，标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。激活函数主要用于提升神经网络解决非线性问题的能力。激活函数各式各样，各有">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/18600497-ebc14f6a033ca8a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">





  
  
  <link rel="canonical" href="https://agentai.github.io/2019/07/09/activation_functions_in_deeplearning/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>深度学习的激活函数 | 多巴胺授体</title>
  






  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f8b52c6d80db5278202277611f7d3554";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>







  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">多巴胺授体</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



</div>
    </header>

    
  
  

  

  <a href="https://github.com/agentai" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://agentai.github.io/2019/07/09/activation_functions_in_deeplearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="duobaan_shouti">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/upload/image/webwxgeticon.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="多巴胺授体">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">深度学习的激活函数

              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-07-09 19:21:24" itemprop="dateCreated datePublished" datetime="2019-07-09T19:21:24+08:00">2019-07-09</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-11 10:35:25" itemprop="dateModified" datetime="2019-07-11T10:35:25+08:00">2019-07-11</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
                 阅读次数： 
                <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
              </span>
            </span>
          

          <br>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              
                <span class="post-meta-item-text">本文字数：</span>
              
              <span title="本文字数">16k</span>
            </span>
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              
              <span title="阅读时长">29 分钟</span>
            </span>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="说明">说明</h1>
<p>本文对深度学习中的重要组件——激活函数做系统性汇总。</p>
<h1 id="了解激活函数">了解激活函数</h1>
<h2 id="什么是激活函数">什么是激活函数</h2>
<p>在神经网络中，一个节点的激活函数(Activation Function)定义了该节点在给定的输入变量或输入集合下的输出。wiki中以计算机芯片电路为例，标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。激活函数主要用于提升神经网络解决非线性问题的能力。激活函数各式各样，各有优缺点，目前常用的有 ReLU、sigmoid、tanh等。各个激活函数的细节详见下文。</p>
<h2 id="为什么需要激活函数">为什么需要激活函数</h2>
<p>当不用激活函数时，神经网络的权重和偏差只会进行<code>线性变换</code>。线性方程很简单，但是解决复杂问题的能力有限。没有激活函数的神经网络实质上就是一个<code>线性回归模型</code>。为了方便理解，以一个简单的例子来说明。考虑如下网络 <img src="https://upload-images.jianshu.io/upload_images/18600497-ebc14f6a033ca8a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="一个简单神经网络图"> 在不用激活函数的情况下，该图可用如下公式表示</p>
<p><span class="math display">\[output = w7(input1*w1 +input2*w2)+w8(input1*w3+input2*w4)+w9(input1*w5+input2*w6)\]</span></p>
<p>实质是<span class="math inline">\(output=\begin{bmatrix} w1*w7+w3*w8+w5*w9 \\ w2*w7+w4*w8+w6*w9 \end{bmatrix} * \begin{bmatrix}input1 \\ input2 \end{bmatrix} \implies Y=WX\)</span>的线性方程。</p>
<p>若在隐藏层引入激活函数<span class="math inline">\(h(y)\)</span>，例如令<span class="math inline">\(h(y) = max(y,0)\)</span>，那么原始式子就无法用简单线性方程表示了。</p>
<p><span class="math display">\[output=w7*max(input1*w1 +input2*w2,0)+w8*max(input1*w3+input2*w4,0)+w9*max(input1*w5+input2*w6,0)\]</span></p>
<h2 id="激活函数类型">激活函数类型</h2>
<p>wiki中给出了激活函数的类型，这里略做归纳<br>
* Identity function (恒等函数)<br>
* Step function (阶跃函数)<br>
* Sigmoidal function (S形函数)<br>
* Ramp function (斜坡函数)</p>
<p>Binary 和 Bipolar 的区别应该是 Binary（单位）对应值为 0 或 1， Bipolar（两极）对应值为 -1 或 1</p>
<h2 id="激活函数的一些特性">激活函数的一些特性</h2>
<ul>
<li><strong>非线性(Nonlinear)</strong> 当激活函数是非线性的，那么一个两层神经网络也证明是一个通用近似函数<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" target="_blank" rel="noopener">通用近似理论</a>。而恒等激活函数则无法满足这一特性，当多层网络的每一层都是恒等激活函数时，该网络实质等同于一个单层网络。<br>
</li>
<li><strong>输出值域(Range)</strong> 若激活函数的值域是有限的，因为对权重的更新影响有限，所以基于梯度的训练方法更稳定。若值域是无限的，因为大多数情况下权重更新更明显，所以训练通常更有效，通常需要设定较小的学习率。<br>
</li>
<li><strong>连续可微(Continuously differentiable)</strong> 通常情况下，当激活函数连续可微，则可以用基于梯度的优化方法。（也有例外，如ReLU函数虽不是连续可微，使用梯度优化也存在一些问题，如ReLU存在由于梯度过大或学习率过大而导致某个神经元输出小于0，从而使得该神经元输出始终是0，并且无法对与其相连的神经元参数进行更新，相当于该神经元进入了“休眠”状态，参考<a href="https://www.zhihu.com/question/67151971" target="_blank" rel="noopener">深度学习中，使用relu存在梯度过大导致神经元“死亡”，怎么理解？ - 知乎</a>，但ReLU还是可以使用梯度优化的。）二值阶跃函数在0处不可微，并且在其他地方的导数是零，所以梯度优化方法不适用于该激活函数。<br>
</li>
<li><strong>单调(Monotonic)</strong> 当激活函数为单调函数时，单层模型的误差曲面一定是凸面。即对应的误差函数是凸函数，求得的最小值一定是全局最小值。<br>
</li>
<li><strong>一阶导单调(Smooth functions with a monotonic derivative)</strong> 通常情况下，这些函数表现更好。<br>
</li>
<li><strong>原点近似恒等函数(Approximates identity near the origin)</strong> 若激活函数有这一特性，神经网络在随机初始化较小的权重时学习更高效。若激活函数不具备这一特性，初始化权重时必须特别小心。参考<a href="https://stats.stackexchange.com/questions/288722/why-activation-functions-that-approximate-the-identity-near-origin-are-preferabl" target="_blank" rel="noopener">machine learning - Why activation functions that approximate the identity near origin are preferable? - Cross Validated</a></li>
</ul>
<h1 id="激活函数速查表">激活函数速查表</h1>
<p>以下是输入为一个变量的激活函数列表，主要参考自 <a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank" rel="noopener">wiki</a></p>
<table style="width:100%;">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">函数</th>
<th style="text-align: center;">图形</th>
<th style="text-align: center;">值域</th>
<th style="text-align: center;">连续性</th>
<th style="text-align: center;">单调</th>
<th style="text-align: center;">一阶导单调</th>
<th style="text-align: center;">原点近似恒等</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Identity(<a href="https://zh.wikipedia.org/wiki/%E6%81%86%E7%AD%89%E5%87%BD%E6%95%B8" title="恒等函数" target="_blank" rel="noopener">恒等函数</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-5710b0fd9c5ee858.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="even">
<td style="text-align: center;">Binary step(<a href="https://zh.wikipedia.org/wiki/%E5%8D%95%E4%BD%8D%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0" title="单位阶跃函数" target="_blank" rel="noopener">单位阶跃函数</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-b2b444d1e96bf8e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle \{0,1\}}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{-1}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Sigmoid(<a href="https://zh.wikipedia.org/wiki/S%E5%87%BD%E6%95%B0" title="S函数" target="_blank" rel="noopener">S函数</a>又称Logistic<a href="https://zh.wikipedia.org/wiki/%E9%82%8F%E8%BC%AF%E5%87%BD%E6%95%B8" title="逻辑函数" target="_blank" rel="noopener">逻辑函数</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-7ecc2d07df55ae67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (0,1)}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">TanH(<a href="https://zh.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0" title="双曲正切函数" target="_blank" rel="noopener">双曲正切函数</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-9210b7ff830fdc94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-1,1)}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">ArcTan(<a href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E6%AD%A3%E5%88%87" title="反正切" target="_blank" rel="noopener">反正切函数</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-03c83295f756f2e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle \left(-{\frac {\pi }{2}},{\frac {\pi }{2}}\right)}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="even">
<td style="text-align: center;">Softsign函数</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-1042e88c2c7c93ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-1,1)}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{1}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Inverse square root unit(反平方根函数,<a href="https://arxiv.org/pdf/1710.09967.pdf" target="_blank" rel="noopener">ISRU</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-4bc4074325eb5c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle \left(-{\frac {1}{\sqrt {\alpha }}},{\frac {1}{\sqrt {\alpha }}}\right)}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="even">
<td style="text-align: center;">Inverse square root linear unit(反平方根线性函数,ISRLU)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-0e78f5b1f5f92c65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle \left(-{\frac {1}{\sqrt {\alpha }}},\infty \right)}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{2}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Square Nonlinearity(平方非线性函数,SQNL)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-245230edcd3a4a9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-1,1)}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{2}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="even">
<td style="text-align: center;">Rectified linear unit(<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0" title="线性整流函数" target="_blank" rel="noopener">线性整流函数</a>,ReLU)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-aaaa71e376ca3391.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle [0,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{0}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Bipolar rectified linear unit(二级线性整流函数,<a href="https://arxiv.org/pdf/1709.04054.pdf" target="_blank" rel="noopener">BReLU</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-c08bbbca6e05005b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{0}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">Leaky rectified linear unit(<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0#%E5%B8%A6%E6%B3%84%E9%9C%B2%E9%9A%8F%E6%9C%BA%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81" title="线性整流函数" target="_blank" rel="noopener">带泄露随机线性整流函数</a>,Leaky ReLU)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-de814e275edfd1de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{0}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Parameteric rectified linear unit(参数化线性整流函数,PReLU)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-193a91a0a6deee5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{0}}\)</span></td>
<td style="text-align: center;">是 <a href="https://en.wikipedia.org/wiki/If_and_only_if" title="If and only if" target="_blank" rel="noopener">iff</a> <span class="math inline">\({\displaystyle \alpha \geq 0}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是 <a href="https://en.wikipedia.org/wiki/If_and_only_if" title="If and only if" target="_blank" rel="noopener">iff</a> <span class="math inline">\({\displaystyle \alpha = 1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Randomized leaky rectified linear unit(<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0#%E5%B8%A6%E6%B3%84%E9%9C%B2%E9%9A%8F%E6%9C%BA%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81" title="线性整流函数" target="_blank" rel="noopener">带泄露随机线性整流函数</a>,RReLU)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-7c8d441bd88e7593.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{0}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Exponential linear unit(指数线性函数,ELU)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-20873dee2d68795d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\alpha ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle {\begin{cases}C_{1}&amp;{\text{when }}\alpha =1\\C_{0}&amp;{\text{otherwise }}\end{cases}}}\)</span></td>
<td style="text-align: center;">是 <a href="https://en.wikipedia.org/wiki/If_and_only_if" title="If and only if" target="_blank" rel="noopener">iff</a> <span class="math inline">\({\displaystyle \alpha \geq 0}\)</span></td>
<td style="text-align: center;">是 <a href="https://en.wikipedia.org/wiki/If_and_only_if" title="If and only if" target="_blank" rel="noopener">iff</a> <span class="math inline">\({\displaystyle 0\leq \alpha \leq 1}\)</span></td>
<td style="text-align: center;">是 <a href="https://en.wikipedia.org/wiki/If_and_only_if" title="If and only if" target="_blank" rel="noopener">iff</a> <span class="math inline">\({\displaystyle \alpha = 1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Scaled exponential linear unit(扩展指数线性函数,SELU)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-813a02444c461bf1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\lambda \alpha ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{0}}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">S-shaped rectified linear activation unit(S型线性整流激活函数,SReLU)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-6b723b46e8e6d58a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{0}}\)</span></td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">Adaptive piecewise linear(自适应分段线性函数,APL)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{0}}\)</span></td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">SoftPlus函数</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-e78b931a88ee79c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (0,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">Bent identity(弯曲恒等函数)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-d48fb0c9bcd7aeb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Sigmoid-weighted linear unit (SiLU)<sup><a href="https://zh.wikipedia.org/wiki/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0#cite_note-11" target="_blank" rel="noopener">[11]</a></sup> (也被称为Swish<sup><a href="https://zh.wikipedia.org/wiki/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0#cite_note-12" target="_blank" rel="noopener">[12]</a></sup>)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle [\approx -0.28,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">SoftExponential函数</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-1d977dd9ebda9431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (-\infty ,\infty )}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">是 <a href="https://en.wikipedia.org/wiki/If_and_only_if" title="If and only if" target="_blank" rel="noopener">iff</a> <span class="math inline">\({\displaystyle \alpha = 0}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Soft Clipping(柔性剪峰函数)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-a090a7783d0d8779.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (0,1)}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Sine_wave" target="_blank" rel="noopener">Sinusoid</a>(<a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%BC%A6%E5%87%BD%E6%95%B0" title="正弦函数" target="_blank" rel="noopener">正弦函数</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-d3e3e1eb128a9880.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle [-1,1]}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><a href="https://zh.wikipedia.org/wiki/Sinc%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">Sinc函数</a></td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-dd49869c7fea9686.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle [\approx -.217234,1]}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a href="https://en.wikipedia.org/wiki/Gaussian_function" title="Gaussian function" target="_blank" rel="noopener">Gaussian</a>(<a href="https://zh.wikipedia.org/wiki/%E9%AB%98%E6%96%AF%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">高斯函数</a>)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-b9849f4e71ae2f9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle (0,1]}\)</span></td>
<td style="text-align: center;"><span class="math inline">\({\displaystyle C^{\infty }}\)</span></td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Hard Sigmoid(分段近似Sigmoid函数)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-2c3e53ea13dc080e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\([0,1]\)</span></td>
<td style="text-align: center;"><span class="math inline">\(C^{0}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">Hard Tanh(分段近似Tanh函数)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-b5d4e0248cb1acfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\([-1,1]\)</span></td>
<td style="text-align: center;"><span class="math inline">\(C^{0}\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">是</td>
</tr>
<tr class="odd">
<td style="text-align: center;">LeCun Tanh(也称Scaled Tanh,按比例缩放的Tanh函数)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-1bb660c33c3059d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\((-1.7519 , 1.7519)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(C^{\infty }\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">Symmetrical Sigmoid(对称Sigmoid函数)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-5aaa82d797f96fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\((-1, -1)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(C^{\infty }\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Complementary Log Log函数</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-08cdebbecc3d058b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\((0, 1)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(C^{\infty }\)</span></td>
<td style="text-align: center;">是</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
<tr class="even">
<td style="text-align: center;">Absolute(绝对值函数)</td>
<td style="text-align: center;"><img src="https://upload-images.jianshu.io/upload_images/18600497-7d837d66e7bac174.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></td>
<td style="text-align: center;"><span class="math inline">\([0, \infty)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(C^{0 }\)</span></td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
<td style="text-align: center;">否</td>
</tr>
</tbody>
</table>
<h1 id="激活函数详细描述">激活函数详细描述</h1>
<p><strong>函数图形说明</strong><br>
函数图形主要取自<a href="%5Bhttps://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/%5D(https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)">Visualising Activation Functions in Neural Networks</a>。<br>
翻译版本访问<a href="/2019/07/09/activation_functions/" title="这里">这里</a> 左侧蓝线是激活函数方程式图形，黄色是激活函数一阶导图形。<br>
右侧篮框是对激活函数方程式的各项特性描述，包括：<br>
- Range(激活函数输出值域)<br>
- Monotonic(激活函数是否单调)<br>
- Continuity(激活函数连续性类型)<br>
- Identity at Origin(激活函数在原点处是否近似恒等)<br>
- Symmetry(激活函数是否对称)</p>
<p>右侧黄框是对激活函数一阶导的各项特性描述，包括：<br>
- Range(一阶导输出值域)<br>
- Monotonic(一阶导是否单调)<br>
- Continuous(一阶导是否连续)<br>
- Vanishing Gradient(是否梯度消失:<em>指随着网络向后传递，是否存在当梯度值过小时，梯度变化以指数形式衰减，直至消失，导致后面的网络节点几乎无法更新参数</em>)<br>
- Exploding Gradient(是否梯度爆炸:<em>指是否存在当梯度值过大，梯度变化以指数形式增加，直至爆炸</em>)<br>
- Saturation(饱和:<em>指激活函数值接近其边界时，如sigmoid函数值接近0或1时，函数曲线是否平缓，过于平缓可能会导致在向后传播时梯度消失</em>)<br>
- Dead Neurons(神经元死亡:<em>指在网络传播时，是否会出现某个神经元永远不会再更新的情况</em>)</p>
<h2 id="identity恒等函数">Identity(恒等函数)</h2>
<p><strong>描述</strong>： 一种输入和输出相等的激活函数，比较适合线性问题，如线性回归问题。但不适用于解决非线性问题。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=x}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=1}\)</span><br>
<strong>图形</strong>：<br>
<img src="https://upload-images.jianshu.io/upload_images/18600497-5710b0fd9c5ee858.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="binary-step单位阶跃函数">Binary step(单位阶跃函数)</h2>
<p><strong>描述</strong>： step与神经元激活的含义最贴近，指当刺激超过阈值时才会激发。但是由于该函数的梯度始终为0，不能作为深度网络的激活函数<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)={\begin{cases}0&amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f^{\prime}(x)={\begin{cases}0&amp;{\text{for }}x\neq 0\\?&amp;{\text{for }}x=0\end{cases}}}\)</span><br>
<strong>图形</strong>：<br>
<img src="https://upload-images.jianshu.io/upload_images/18600497-b2b444d1e96bf8e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="sigmoids函数又称logistic逻辑函数">Sigmoid(S函数又称Logistic逻辑函数)</h2>
<p><strong>描述</strong>： 使用很广的一类激活函数，具有指数函数形状，在物理意义上最接近生物神经元。并且值域在(0,1)之间，可以作为概率表示。该函数也通常用于对输入的归一化，如Sigmoid交叉熵损失函数。Sigmoid激活函数具有梯度消失和饱和的问题，一般来说，sigmoid网络在5层之内就会产生梯度消失现象。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=\sigma (x)={\frac {1}{1+e^{-x}}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=f(x)(1-f(x))}\)</span><br>
<strong>图形</strong>：<br>
<img src="https://upload-images.jianshu.io/upload_images/18600497-7ecc2d07df55ae67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="tanh双曲正切函数">TanH(双曲正切函数)</h2>
<p><strong>描述</strong>： TanH与Sigmoid函数类似，在输入很大或很小时，输出几乎平滑，梯度很小，不利于权重更新，容易出现梯度消失和饱和的问题。不过TanH函数值域在(-1,1)之间，以0为中心反对称，且原点近似恒等，这些点是加分项。一般二分类问题中，隐藏层用tanh函数，输出层用sigmod函数。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=\tanh(x)={\frac {(e^{x}-e^{-x})}{(e^{x}+e^{-x})}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=1-f(x)^{2}}\)</span><br>
<strong>图形</strong>：<br>
<img src="https://upload-images.jianshu.io/upload_images/18600497-9210b7ff830fdc94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="arctan反正切函数">ArcTan(反正切函数)</h2>
<p><strong>描述</strong>： ArcTen从图形上看类似TanH函数，只是比TanH平缓，值域更大。从一阶导看出导数趋于零的速度比较慢，因此训练比较快。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=\tan ^{-1}(x)}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)={\frac {1}{x^{2}+1}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-03c83295f756f2e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="softsign函数">Softsign函数</h2>
<p><strong>描述</strong>： Softsign从图形上看也类似TanH函数，以0为中心反对称，训练比较快。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)={\frac {x}{1+\|x\|}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)={\frac {1}{(1+\|x\|)^{2}}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-1042e88c2c7c93ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="inverse-square-root-unit反平方根函数isru">Inverse square root unit(反平方根函数,<a href="https://arxiv.org/pdf/1710.09967.pdf" target="_blank" rel="noopener">ISRU</a>)</h2>
<p><strong>描述</strong>： 图形类似于tanH和Sigmoid函数，论文说可作为RNN层的激活函数，号称在RNN层中达到与tanh和sigmoid一样效果的情况下，计算复杂度更低。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)={\frac {x}{\sqrt {1+\alpha x^{2}}}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=\left({\frac {1}{\sqrt {1+\alpha x^{2}}}}\right)^{3}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-4bc4074325eb5c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="inverse-square-root-linear-unit反平方根线性函数isrlu">Inverse square root linear unit(反平方根线性函数,ISRLU)</h2>
<p><strong>描述</strong>： 一个分段函数，小于0部分是ISRU，大于等于0部分是恒等函数，论文里说这函数在CNN层上相较于ReLU学习更快，更一般化。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)={\begin{cases}{\frac {x}{\sqrt {1+\alpha x^{2}}}}&amp;{\text{for }}x&lt;0\\x&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)={\begin{cases}\left({\frac {1}{\sqrt {1+\alpha x^{2}}}}\right)^{3}&amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-0e78f5b1f5f92c65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="square-nonlinearity平方非线性函数sqnl">Square Nonlinearity(平方非线性函数,SQNL)</h2>
<p><strong>描述</strong>： 类似Softsign的函数，<a href="https://ieeexplore.ieee.org/document/8489043" target="_blank" rel="noopener">论文摘要</a>中将这个与Softsign对比，说该函数在收敛速度上更快。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)={\begin{cases}1&amp;:x&gt;2.0\\x-{\frac {x^{2}}{4}}&amp;:0\leq x\leq 2.0\\x+{\frac {x^{2}}{4}}&amp;:-2.0\leq x&lt;0\\-1&amp;:x&lt;-2.0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=1\mp {\frac {x}{2}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-245230edcd3a4a9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="rectified-linear-unit线性整流函数relu">Rectified linear unit(<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0" title="线性整流函数" target="_blank" rel="noopener">线性整流函数</a>,ReLU)</h2>
<p><strong>描述</strong>： 比较流行的激活函数，该函数保留了类似step那样的生物学神经元机制，即高于0才激活，不过因在0以下的导数都是0，可能会引起学习缓慢甚至神经元死亡的情况。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)={\begin{cases}0&amp;{\text{for }}x\leq 0\\x&amp;{\text{for }}x&gt;0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)={\begin{cases}0&amp;{\text{for }}x\leq 0\\1&amp;{\text{for }}x&gt;0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-aaaa71e376ca3391.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="bipolar-rectified-linear-unit二级线性整流函数brelu">Bipolar rectified linear unit(二级线性整流函数,<a href="https://arxiv.org/pdf/1709.04054.pdf" target="_blank" rel="noopener">BReLU</a>)</h2>
<p><strong>描述</strong>： relu系列的一种激活函数，形式很奇怪，采用mod 2作为条件分段，论文说在个别场景上能取得更好的效果。<br>
<strong>方程式</strong>： <span class="math inline">\(f(x_{i})={\begin{cases}ReLU(x_{i})&amp;{\text{if }}i{\bmod {2}}=0\\-ReLU(-x_{i})&amp;{\text{if }}i{\bmod {2}}\neq 0\end{cases}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x_{i})={\begin{cases}ReLU&#39;(x_{i})&amp;{\text{if }}i{\bmod {2}}=0\\-ReLU&#39;(-x_{i})&amp;{\text{if }}i{\bmod {2}}\neq 0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-c08bbbca6e05005b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="leaky-rectified-linear-unit带泄露随机线性整流函数leaky-relu">Leaky rectified linear unit(<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0#%E5%B8%A6%E6%B3%84%E9%9C%B2%E9%9A%8F%E6%9C%BA%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81" title="线性整流函数" target="_blank" rel="noopener">带泄露随机线性整流函数</a>,Leaky ReLU)</h2>
<p><strong>描述</strong>： relu的一个变化，即在小于0部分不等于0，而是加一个很小的不为零的斜率，减少神经元死亡带来的影响。<br>
<strong>方程式</strong>： <span class="math inline">\({ f(x)={\begin{cases}0.01x&amp;{\text{for }}x&lt;0\\x&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)={\begin{cases}0.01&amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-de814e275edfd1de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="parameteric-rectified-linear-unit参数化线性整流函数prelu">Parameteric rectified linear unit(参数化线性整流函数,PReLU)</h2>
<p><strong>描述</strong>： 也是ReLU的一个变化，与Leaky ReLU类似，只不过PReLU将小于零部分的斜率换成了可变参数α。这种变化使值域会依据α不同而不同。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(\alpha ,x)={\begin{cases}\alpha x&amp;{\text{for }}x&lt;0\\x&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({ f&#39;(\alpha ,x)={\begin{cases}\alpha &amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-193a91a0a6deee5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="randomized-leaky-rectified-linear-unit带泄露随机线性整流函数rrelu">Randomized leaky rectified linear unit(<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0#%E5%B8%A6%E6%B3%84%E9%9C%B2%E9%9A%8F%E6%9C%BA%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81" title="线性整流函数" target="_blank" rel="noopener">带泄露随机线性整流函数</a>,RReLU)</h2>
<p><strong>描述</strong>： 在PReLU基础上将α变成了随机数。参考<a href="https://arxiv.org/abs/1505.00853" target="_blank" rel="noopener">论文</a><br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(\alpha ,x)={\begin{cases}\alpha x&amp;{\text{for }}x&lt;0\\x&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({ f&#39;(\alpha ,x)={\begin{cases}\alpha &amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-7c8d441bd88e7593.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="exponential-linear-unit指数线性函数elu">Exponential linear unit(指数线性函数,ELU)</h2>
<p><strong>描述</strong>： ELU小于零的部分采用了负指数形式，相较于ReLU权重可以有负值，并且在输入取较小值时具有软饱和的特性，提升了对噪声的鲁棒性。参考<a href="https://blog.csdn.net/mao_xiao_feng/article/details/53242235?locationNum=9&amp;fps=1" target="_blank" rel="noopener">ELU激活函数的提出</a><br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(\alpha ,x)={\begin{cases}\alpha (e^{x}-1)&amp;{\text{for }}x\leq 0\\x&amp;{\text{for }}x&gt;0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({ f&#39;(\alpha ,x)={\begin{cases}f(\alpha ,x)+\alpha &amp;{\text{for }}x\leq 0\\1&amp;{\text{for }}x&gt;0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-20873dee2d68795d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="scaled-exponential-linear-unit扩展指数线性函数selu">Scaled exponential linear unit(扩展指数线性函数,SELU)</h2>
<p><strong>描述</strong>： ELU的一种变化，引入超参λ和α，并给出了相应取值，这些取值在原论文中（<a href="https://arxiv.org/abs/1706.02515" target="_blank" rel="noopener">Self-Normalizing Neural Networks</a>）详细推导过程<br>
<strong>方程式</strong>： ${ f(,x)=} $<br>
<span class="math inline">\(with \quad{\displaystyle \lambda =1.0507} \quad and \quad {\displaystyle \alpha =1.67326}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(\alpha ,x)=\lambda {\begin{cases}\alpha (e^{x})&amp;{\text{for }}x&lt;0\\1&amp;{\text{for }}x\geq 0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-813a02444c461bf1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="s-shaped-rectified-linear-activation-units型线性整流激活函数srelu">S-shaped rectified linear activation unit(S型线性整流激活函数,SReLU)</h2>
<p><strong>描述</strong>： 也是ReLU的一种变化，不同的是该函数有三个分段，四个超参数，这种设置使函数图形看起来像S型，具体参考<a href="https://arxiv.org/abs/1512.07030" target="_blank" rel="noopener">论文</a>。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f_{t_{l},a_{l},t_{r},a_{r}}(x)={\begin{cases}t_{l}+a_{l}(x-t_{l})&amp;{\text{for }}x\leq t_{l}\\x&amp;{\text{for }}t_{l}&lt;x&lt;t_{r}\\t_{r}+a_{r}(x-t_{r})&amp;{\text{for }}x\geq t_{r}\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;_{t_{l},a_{l},t_{r},a_{r}}(x)={\begin{cases}a_{l}&amp;{\text{for }}x\leq t_{l}\\1&amp;{\text{for }}t_{l}&lt;x&lt;t_{r}\\a_{r}&amp;{\text{for }}x\geq t_{r}\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-6b723b46e8e6d58a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="adaptive-piecewise-linear自适应分段线性函数apl">Adaptive piecewise linear(自适应分段线性函数,APL)</h2>
<p><strong>描述</strong>： 原<a href="https://arxiv.org/abs/1412.6830" target="_blank" rel="noopener">论文</a>表示通过分段为神经元学习加入自适应激活功能，比ReLU在cifar-10、cifar-100上有更好的性能。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=\max(0,x)+\sum _{s=1}^{S}a_{i}^{s}\max(0,-x+b_{i}^{s})}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=H(x)-\sum _{s=1}^{S}a_{i}^{s}H(-x+b_{i}^{s})}\)</span><br>
<strong>图形</strong>：</p>
<h2 id="softplus函数">SoftPlus函数</h2>
<p><strong>描述</strong>： 是ReLU的平滑替代，函数在任何地方连续且值域非零，避免了死神经元。不过因不对称且不以零为中心，可以影响网络学习。由于导数必然小于1，所以也存在梯度消失问题。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=\ln(1+e^{x})}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)={\frac {1}{1+e^{-x}}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-e78b931a88ee79c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="bent-identity弯曲恒等函数">Bent identity(弯曲恒等函数)</h2>
<p><strong>描述</strong>： 可以理解为identity和ReLU之间的一种折中，不会出现死神经元的问题，不过存在梯度消失和梯度爆炸风险。<br>
<strong>方程式</strong>： <span class="math inline">\(\begin{align*}f(x)=\frac{\sqrt{x^2 + 1} - 1}{2} + x \end{align*}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)={\frac {x}{2{\sqrt {x^{2}+1}}}}+1}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-d48fb0c9bcd7aeb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="sigmoid-weighted-linear-unit-silu">Sigmoid-weighted linear unit (SiLU)</h2>
<p><strong>描述</strong>： 具体参考<a href="https://arxiv.org/abs/1702.03118" target="_blank" rel="noopener">论文</a><br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=x\cdot \sigma (x)}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=f(x)+\sigma (x)(1-f(x))}\)</span><br>
<strong>图形</strong>：</p>
<h2 id="softexponential函数">SoftExponential函数</h2>
<p><strong>描述</strong>： 参考<a href="https://arxiv.org/abs/1602.01321" target="_blank" rel="noopener">论文</a><br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(\alpha ,x)={\begin{cases}-{\frac {\ln(1-\alpha (x+\alpha ))}{\alpha }}&amp;{\text{for }}\alpha &lt;0\\x&amp;{\text{for }}\alpha =0\\{\frac {e^{\alpha x}-1}{\alpha }}+\alpha &amp;{\text{for }}\alpha &gt;0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(\alpha ,x)={\begin{cases}{\frac {1}{1-\alpha (\alpha +x)}}&amp;{\text{for }}\alpha &lt;0\\e^{\alpha x}&amp;{\text{for }}\alpha \geq 0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-1d977dd9ebda9431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="soft-clipping柔性剪峰函数">Soft Clipping(柔性剪峰函数)</h2>
<p><strong>描述</strong>： 参考<a href="https://arxiv.org/abs/1810.11509" target="_blank" rel="noopener">论文</a><br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(\alpha ,x)={\frac {1}{\alpha }}\log {\frac {1+e^{\alpha x}}{1+e^{\alpha (x-1)}}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(\alpha ,x)={\frac {1}{2}}\sinh \left({\frac {p}{2}}\right){\text{sech}}\left({\frac {px}{2}}\right){\text{sech}}\left({\frac {p}{2}}(1-x)\right)}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-a090a7783d0d8779.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="sinusoid正弦函数"><a href="https://en.wikipedia.org/wiki/Sine_wave" target="_blank" rel="noopener">Sinusoid</a>(<a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%BC%A6%E5%87%BD%E6%95%B0" title="正弦函数" target="_blank" rel="noopener">正弦函数</a>)</h2>
<p><strong>描述</strong>： Sinusoid作为激活函数，为神经网络引入了周期性，且该函数处处联系，以零点对称。参考<a href="https://arxiv.org/abs/1405.2262" target="_blank" rel="noopener">论文</a><br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=\sin(x)}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=\cos(x)}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-d3e3e1eb128a9880.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="sinc函数"><a href="https://zh.wikipedia.org/wiki/Sinc%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">Sinc函数</a></h2>
<p><strong>描述</strong>： Sinc函数在信号处理中尤为重要，因为它表征了矩形函数的傅立叶变换。作为激活函数，它的优势在于处处可微和对称的特性，不过容易产生梯度消失的问题。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)={\begin{cases}1&amp;{\text{for }}x=0\\{\frac {\sin(x)}{x}}&amp;{\text{for }}x\neq 0\end{cases}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)={\begin{cases}0&amp;{\text{for }}x=0\\{\frac {\cos(x)}{x}}-{\frac {\sin(x)}{x^{2}}}&amp;{\text{for }}x\neq 0\end{cases}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-dd49869c7fea9686.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="gaussian高斯函数"><a href="https://en.wikipedia.org/wiki/Gaussian_function" title="Gaussian function" target="_blank" rel="noopener">Gaussian</a>(<a href="https://zh.wikipedia.org/wiki/%E9%AB%98%E6%96%AF%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">高斯函数</a>)</h2>
<p><strong>描述</strong>： 高斯激活函数不常用。<br>
<strong>方程式</strong>： <span class="math inline">\({\displaystyle f(x)=e^{-x^{2}}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\({\displaystyle f&#39;(x)=-2xe^{-x^{2}}}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-b9849f4e71ae2f9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="hard-sigmoid分段近似sigmoid函数">Hard Sigmoid(分段近似Sigmoid函数)</h2>
<p><strong>描述</strong>： 是Sigmoid函数的分段线性近似，更容易计算，不过存在梯度消失和神经元死亡的问题<br>
<strong>方程式</strong>： <span class="math inline">\(f(x) =\begin{cases}0 &amp; \text{for } x&lt;-2.5\\0.2x + 0.5 &amp; \text{for } -2.5\geq x\leq 2.5 \\1 &amp; \text{for } x&gt;2.5\end{cases}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\(f&#39;(x) =\begin{cases}0 &amp; \text{for } x&lt;-2.5\\0.2 &amp; \text{for } -2.5\geq x\leq 2.5 \\0 &amp; \text{for } x&gt;2.5\end{cases}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-2c3e53ea13dc080e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="hard-tanh分段近似tanh函数">Hard Tanh(分段近似Tanh函数)</h2>
<p><strong>描述</strong>： Tanh激活函数的分段线性近似。<br>
<strong>方程式</strong>： <span class="math inline">\(f(x) =\begin{cases}-1 &amp; \text{for } x&lt;-1\\x &amp; \text{for } -1\geq x\leq 1 \\1 &amp; \text{for } x&gt;1 \end{cases}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\(f&#39;(x) =\begin{cases}0 &amp; \text{for } x&lt;-1\\1 &amp; \text{for } -1\geq x\leq 1 \\0 &amp; \text{for } x&gt;1\end{cases}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-b5d4e0248cb1acfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="lecun-tanh也称scaled-tanh按比例缩放的tanh函数">LeCun Tanh(也称Scaled Tanh,按比例缩放的Tanh函数)</h2>
<p><strong>描述</strong>： Tanh的缩放版本，参考<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener">论文</a><br>
<strong>方程式</strong>： <span class="math inline">\(\begin{align*}f(x)&amp;=1.7519\tanh(\frac{2}{3}x)\end{align*}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\(\begin{align*}f&#39;(x)&amp;=1.7519*\frac{2}{3}(1-\tanh^2(\frac{2}{3}x))\\&amp;=1.7519*\frac{2}{3}-\frac{2}{3*1.7519}f(x)^2\end{align*}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-1bb660c33c3059d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="symmetrical-sigmoid对称sigmoid函数">Symmetrical Sigmoid(对称Sigmoid函数)</h2>
<p><strong>描述</strong>： 是Tanh的一种替代方法，比Tanh形状更扁平，导数更小，下降更缓慢。<br>
<strong>方程式</strong>： <span class="math inline">\(\begin{align*}f(x)&amp;=\tanh(x/2)\\&amp;=\frac{1-e^{-x}}{1+e^{-x}}\end{align*}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\(\begin{align*}f&#39;(x)&amp;=0.5(1-\tanh^2(x/2))\\&amp;=0.5(1-f(x)^2)\end{align*}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-5aaa82d797f96fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="complementary-log-log函数">Complementary Log Log函数</h2>
<p><strong>描述</strong>： 是Sigmoid的一种替代，相较于Sigmoid更饱和。<br>
<strong>方程式</strong>： <span class="math inline">\(f(x)=1-e^{-e^{x}}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\(\begin{align*}f&#39;(x)=e^{x}(e^{-e^{x}}) = e^{x-e^{x}}\end{align*}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-08cdebbecc3d058b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h2 id="absolute绝对值函数">Absolute(绝对值函数)</h2>
<p><strong>描述</strong>： 导数只有两个值。<br>
<strong>方程式</strong>： <span class="math inline">\(\begin{align*}f(x)=|x|\end{align*}\)</span><br>
<strong>一阶导</strong>： <span class="math inline">\(f&#39;(x) =\begin{cases}-1 &amp; \text{for } x&lt;0\\1 &amp; \text{for } x&gt;0\\? &amp; \text{for } x=0\end{cases}\)</span><br>
<strong>图形</strong>：<img src="https://upload-images.jianshu.io/upload_images/18600497-7d837d66e7bac174.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<h1 id="附录">附录</h1>
<h2 id="order-of-continuity函数连续性">Order of continuity(函数连续性)</h2>
<p>函数参数连续性是个人的意译，大概意思应该是激活函数曲线的连续性类型。wiki中有对各个值的说明，可参考<a href="https://en.wikipedia.org/wiki/Smoothness#Order_of_continuity" target="_blank" rel="noopener">Order_of_continuity</a>和<a href="https://blog.csdn.net/weixin_42513339/article/details/83474118" target="_blank" rel="noopener">描述光滑度中对于曲线、曲面的C0、C1、C2和G0、G1、G2定义和区别 - chenchen_fcj程序猿的博客 - CSDN博客</a>，简单说明如下</p>
<ul>
<li><p><span class="math inline">\(C^{-1}\)</span>: 曲线不连续</p></li>
<li><p><span class="math inline">\(C^{0}\)</span>: 曲线本身是连续的，但不可导</p></li>
<li><p><span class="math inline">\(C^{1}\)</span>:  曲线一阶导是连续的，但无法二阶导</p></li>
<li><p><span class="math inline">\(C^{2}\)</span>:  曲线一阶导和二阶导是连续的，但无法三阶导</p></li>
<li><p><span class="math inline">\(C^{n}\)</span>:  曲线一阶导到n阶导是连续的，但无法进行n+1阶导</p></li>
<li><p><span class="math inline">\(C^{\infty}\)</span>: 曲线无穷阶可导</p></li>
</ul>
<h1 id="参考">参考</h1>
<p><a href="https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/" target="_blank" rel="noopener">visualising-activation-functions-in-neural-networks</a><br>
<a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank" rel="noopener">Activation_function</a><br>
<a href="https://zhuanlan.zhihu.com/p/32824193" class="uri" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32824193</a></p>

      
    </div>

    

<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

  
</div>

    
    
    

    

    
      
    
    
      <div>
        <div id="reward-container">
  <div>🐶 如果对您有帮助的话，赏个咖啡呗 🐶</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">

    
      
      
        
      
      <div style="display: inline-block">
        <img src="/upload/image/weixinzhifu.jpeg" alt="duobaan_shouti 微信支付">
        <p>微信支付</p>
      </div>
    
      
      
        
      
      <div style="display: inline-block">
        <img src="/upload/image/zhifubaozhifu.jpeg" alt="duobaan_shouti 支付宝">
        <p>支付宝</p>
      </div>
    

  </div>
</div>

      </div>
    

    
      <div>
        




  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>duobaan_shouti</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    
    <a href="https://agentai.github.io/2019/07/09/activation_functions_in_deeplearning/" title="深度学习的激活函数">https://agentai.github.io/2019/07/09/activation_functions_in_deeplearning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
          
            <a href="/tags/激活函数/" rel="tag"><i class="fa fa-tag"></i> 激活函数</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div class="social_share">
            
            
            
              <div>
                
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

              </div>
            
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/09/activation_functions/" rel="next" title="激活函数图形">
                <i class="fa fa-chevron-left"></i> 激活函数图形
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/11/机器学习资料/" rel="prev" title="机器学习资料">
                机器学习资料 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="gitalk-container">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/upload/image/webwxgeticon.jpeg" alt="duobaan_shouti">
            
              <p class="site-author-name" itemprop="name">duobaan_shouti</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/agentai" title="GitHub &rarr; https://github.com/agentai" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          
             <div class="cc-license motion-element" itemprop="license">
              
              
                
              
              
              
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
             </div>
          

          
          

          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#说明"><span class="nav-number">1.</span> <span class="nav-text">说明</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#了解激活函数"><span class="nav-number">2.</span> <span class="nav-text">了解激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是激活函数"><span class="nav-number">2.1.</span> <span class="nav-text">什么是激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么需要激活函数"><span class="nav-number">2.2.</span> <span class="nav-text">为什么需要激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数类型"><span class="nav-number">2.3.</span> <span class="nav-text">激活函数类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数的一些特性"><span class="nav-number">2.4.</span> <span class="nav-text">激活函数的一些特性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#激活函数速查表"><span class="nav-number">3.</span> <span class="nav-text">激活函数速查表</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#激活函数详细描述"><span class="nav-number">4.</span> <span class="nav-text">激活函数详细描述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#identity恒等函数"><span class="nav-number">4.1.</span> <span class="nav-text">Identity(恒等函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#binary-step单位阶跃函数"><span class="nav-number">4.2.</span> <span class="nav-text">Binary step(单位阶跃函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoids函数又称logistic逻辑函数"><span class="nav-number">4.3.</span> <span class="nav-text">Sigmoid(S函数又称Logistic逻辑函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tanh双曲正切函数"><span class="nav-number">4.4.</span> <span class="nav-text">TanH(双曲正切函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#arctan反正切函数"><span class="nav-number">4.5.</span> <span class="nav-text">ArcTan(反正切函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softsign函数"><span class="nav-number">4.6.</span> <span class="nav-text">Softsign函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inverse-square-root-unit反平方根函数isru"><span class="nav-number">4.7.</span> <span class="nav-text">Inverse square root unit(反平方根函数,ISRU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inverse-square-root-linear-unit反平方根线性函数isrlu"><span class="nav-number">4.8.</span> <span class="nav-text">Inverse square root linear unit(反平方根线性函数,ISRLU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#square-nonlinearity平方非线性函数sqnl"><span class="nav-number">4.9.</span> <span class="nav-text">Square Nonlinearity(平方非线性函数,SQNL)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rectified-linear-unit线性整流函数relu"><span class="nav-number">4.10.</span> <span class="nav-text">Rectified linear unit(线性整流函数,ReLU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bipolar-rectified-linear-unit二级线性整流函数brelu"><span class="nav-number">4.11.</span> <span class="nav-text">Bipolar rectified linear unit(二级线性整流函数,BReLU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#leaky-rectified-linear-unit带泄露随机线性整流函数leaky-relu"><span class="nav-number">4.12.</span> <span class="nav-text">Leaky rectified linear unit(带泄露随机线性整流函数,Leaky ReLU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#parameteric-rectified-linear-unit参数化线性整流函数prelu"><span class="nav-number">4.13.</span> <span class="nav-text">Parameteric rectified linear unit(参数化线性整流函数,PReLU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#randomized-leaky-rectified-linear-unit带泄露随机线性整流函数rrelu"><span class="nav-number">4.14.</span> <span class="nav-text">Randomized leaky rectified linear unit(带泄露随机线性整流函数,RReLU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#exponential-linear-unit指数线性函数elu"><span class="nav-number">4.15.</span> <span class="nav-text">Exponential linear unit(指数线性函数,ELU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scaled-exponential-linear-unit扩展指数线性函数selu"><span class="nav-number">4.16.</span> <span class="nav-text">Scaled exponential linear unit(扩展指数线性函数,SELU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#s-shaped-rectified-linear-activation-units型线性整流激活函数srelu"><span class="nav-number">4.17.</span> <span class="nav-text">S-shaped rectified linear activation unit(S型线性整流激活函数,SReLU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adaptive-piecewise-linear自适应分段线性函数apl"><span class="nav-number">4.18.</span> <span class="nav-text">Adaptive piecewise linear(自适应分段线性函数,APL)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softplus函数"><span class="nav-number">4.19.</span> <span class="nav-text">SoftPlus函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bent-identity弯曲恒等函数"><span class="nav-number">4.20.</span> <span class="nav-text">Bent identity(弯曲恒等函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoid-weighted-linear-unit-silu"><span class="nav-number">4.21.</span> <span class="nav-text">Sigmoid-weighted linear unit (SiLU)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softexponential函数"><span class="nav-number">4.22.</span> <span class="nav-text">SoftExponential函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#soft-clipping柔性剪峰函数"><span class="nav-number">4.23.</span> <span class="nav-text">Soft Clipping(柔性剪峰函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sinusoid正弦函数"><span class="nav-number">4.24.</span> <span class="nav-text">Sinusoid(正弦函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sinc函数"><span class="nav-number">4.25.</span> <span class="nav-text">Sinc函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gaussian高斯函数"><span class="nav-number">4.26.</span> <span class="nav-text">Gaussian(高斯函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hard-sigmoid分段近似sigmoid函数"><span class="nav-number">4.27.</span> <span class="nav-text">Hard Sigmoid(分段近似Sigmoid函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hard-tanh分段近似tanh函数"><span class="nav-number">4.28.</span> <span class="nav-text">Hard Tanh(分段近似Tanh函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecun-tanh也称scaled-tanh按比例缩放的tanh函数"><span class="nav-number">4.29.</span> <span class="nav-text">LeCun Tanh(也称Scaled Tanh,按比例缩放的Tanh函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#symmetrical-sigmoid对称sigmoid函数"><span class="nav-number">4.30.</span> <span class="nav-text">Symmetrical Sigmoid(对称Sigmoid函数)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#complementary-log-log函数"><span class="nav-number">4.31.</span> <span class="nav-text">Complementary Log Log函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#absolute绝对值函数"><span class="nav-number">4.32.</span> <span class="nav-text">Absolute(绝对值函数)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附录"><span class="nav-number">5.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#order-of-continuity函数连续性"><span class="nav-number">5.1.</span> <span class="nav-text">Order of continuity(函数连续性)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">6.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">duobaan_shouti</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">站点总字数：</span>
    
    <span title="站点总字数">21k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    
    <span title="站点阅读时长">38 分钟</span>
  
</div>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.2.0"></script>

  <script src="/js/motion.js?v=7.2.0"></script>



  
  


  <script src="/js/affix.js?v=7.2.0"></script>

  <script src="/js/schemes/pisces.js?v=7.2.0"></script>




  
  <script src="/js/scrollspy.js?v=7.2.0"></script>
<script src="/js/post-details.js?v=7.2.0"></script>



  


  <script src="/js/next-boot.js?v=7.2.0"></script>


  

  

  

  


  
    

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">



<script src="//cdn.jsdelivr.net/npm/js-md5@0.7.3/src/md5.min.js"></script>

<script>
  var gitalk = new Gitalk({
    clientID: 'bb9d9950d51d1cb77271',
    clientSecret: 'e0146ed888ae97d4ad2c9efa30a53cd17fab8cc1',
    repo: 'agentai.github.io',
    owner: 'agentai',
    admin: ['agentai'],
    id: md5(location.pathname),
    
      language: window.navigator.language || window.navigator.userLanguage,
    
    distractionFreeMode: 'true'
  });
  gitalk.render('gitalk-container');
</script>

  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  
    MathJax.Ajax.config.path['mhchem'] = '//cdn.jsdelivr.net/npm/mathjax-mhchem@3';
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
        extensions: ['[mhchem]/mhchem.js'],
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src></script>

    
  


  
<script>
if ($('body').find('div.pdf').length) {
  $.ajax({
    type: 'GET',
    url: '//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js',
    dataType: 'script',
    cache: true,
    success: function() {
      $('body').find('div.pdf').each(function(i, o) {
        PDFObject.embed($(o).attr('target'), $(o), {
          pdfOpenParams: {
            navpanes: 0,
            toolbar: 0,
            statusbar: 0,
            pagemode: 'thumbs',
            view: 'FitH'
          },
          PDFJS_URL: '/lib/pdf/web/viewer.html',
          height: $(o).attr('height') || '500px'
        });
      });
    },
  });
}
</script>


  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  

  
<script>
  $('.highlight').not('.gist .highlight').each(function(i, e) {
    var $wrap = $('<div>').addClass('highlight-wrap');
    $(e).after($wrap);
    $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
      var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
        return $(e).text();
      }).toArray().join('\n');
      var ta = document.createElement('textarea');
      var yPosition = window.pageYOffset || document.documentElement.scrollTop;
      ta.style.top = yPosition + 'px'; // Prevent page scroll
      ta.style.position = 'absolute';
      ta.style.opacity = '0';
      ta.readOnly = true;
      ta.value = code;
      document.body.appendChild(ta);
      const selection = document.getSelection();
      const selected = selection.rangeCount > 0 ? selection.getRangeAt(0) : false;
      ta.select();
      ta.setSelectionRange(0, code.length);
      ta.readOnly = false;
      var result = document.execCommand('copy');
      
        if (result) $(this).text('复制成功');
        else $(this).text('复制失败');
      
      ta.blur(); // For iOS
      $(this).blur();
      if (selected) {
        selection.removeAllRanges();
        selection.addRange(selected);
      }
    })).on('mouseleave', function(e) {
      var $b = $(this).find('.copy-btn');
      setTimeout(function() {
        $b.text('复制');
      }, 300);
    }).append(e);
  })
</script>


  
	
	<script>
		(function(d, w, c) {
			w.ChatraID = 'c7SEMuADFZghJGvQt';
			var s = d.createElement('script');
			w[c] = w[c] || function() {
				(w[c].q = w[c].q || []).push(arguments);
			};
			s.async = true;
			s.src = 'https://call.chatra.io/chatra.js';
			if (d.head) d.head.appendChild(s);
		})(document, window, 'Chatra');
	</script>


  


  
<script src="/live2d-widget/autoload.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
